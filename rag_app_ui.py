import streamlit as st
import torch
import hashlib
import os
import json
import re
from pathlib import Path
from pdf2image import convert_from_bytes
import pytesseract
from concurrent.futures import ThreadPoolExecutor

# ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† ØªÙˆØ§Ø¨Ø¹ Ø§Ø² Ø¯Ùˆ ÙØ§ÛŒÙ„ Ø¯ÛŒÚ¯Ø±
from setup_models import setup_llm_and_embeddings
from vector_manager import load_vectorstore_on_gpu, search_documents # <--- Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯

from langchain_community.vectorstores import FAISS
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø³ÛŒØ±Ù‡Ø§
os.environ['TESSDATA_PREFIX'] = os.path.abspath("./models/")
DATA_DIR = Path("data")
OCR_DIR = DATA_DIR / "ocr_texts"
METADATA_DIR = DATA_DIR / "metadata"
INDEX_PATH = "models/faiss_index" # Ù…Ø³ÛŒØ± Ø°Ø®ÛŒØ±Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø±ÙˆÛŒ Ù‡Ø§Ø±Ø¯
OCR_DIR.mkdir(parents=True, exist_ok=True)
METADATA_DIR.mkdir(parents=True, exist_ok=True)

st.set_page_config(page_title="Ø³Ø§Ù…Ø§Ù†Ù‡ ØªØ­Ù„ÛŒÙ„ Ø§Ø³Ù†Ø§Ø¯ Enterprise", layout="wide")

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§
embeddings, llm_engine, prompt_template, (rerank_model, rerank_tokenizer) = setup_llm_and_embeddings()

# --- ØªÙˆØ§Ø¨Ø¹ Ø³ÛŒØ³ØªÙ…ÛŒ ---

def clean_text(text):
    text = text.replace("ÛŒ", "ÛŒ").replace("Ú©", "Ú©")
    text = re.sub(r'[^\u0600-\u06FF\s\d.,;?!()\-]', ' ', text)
    return " ".join(text.split())

def process_single_page(args):
    idx, image = args
    raw = pytesseract.image_to_string(image, lang="fas")
    return idx + 1, clean_text(raw)

def index_documents_from_disk(_embeddings):
    """Ø§Ø³Ú©Ù† Ù‡Ø§Ø±Ø¯ØŒ Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³ Ùˆ Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ù‡ GPU Ø§Ø² Ø·Ø±ÛŒÙ‚ vector_manager"""
    all_docs = []
    splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=250)

    meta_files = list(METADATA_DIR.glob("*.json"))
    if not meta_files: return None

    for meta_file in meta_files:
        try:
            with open(meta_file, "r", encoding="utf-8") as f:
                meta = json.load(f)
            ocr_path = Path(meta["ocr_text_path"])
            if ocr_path.exists():
                with open(ocr_path, "r", encoding="utf-8") as f:
                    text = f.read()
                if text.strip():
                    for chunk in splitter.split_text(text):
                        all_docs.append(Document(page_content=chunk, metadata={"source": meta["original_filename"]}))
        except: continue

    if all_docs:
        # Û±. Ø³Ø§Ø®Øª Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù…ÙˆÙ‚Øª Ø¯Ø± Ø±Ù…
        vs = FAISS.from_documents(all_docs, _embeddings)
        # Û². Ø°Ø®ÛŒØ±Ù‡ Ø±ÙˆÛŒ Ù‡Ø§Ø±Ø¯ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø¹Ø¯ÛŒ
        vs.save_local(INDEX_PATH)
        # Û³. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙØ§ÛŒÙ„ vector_manager Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ù‡ GPU 1 Ùˆ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡
        vs_gpu = load_vectorstore_on_gpu(INDEX_PATH, _embeddings)
        return vs_gpu
    return None

def apply_reranking(query, documents):
    if not documents: return []
    pairs = [[query, doc.page_content] for doc in documents]
    device = next(rerank_model.parameters()).device
    with torch.no_grad():
        inputs = rerank_tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512).to(device)
        scores = rerank_model(**inputs).logits.view(-1,).float()
        combined = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)
        return [doc for doc, score in combined[:8]]

# --- Ø´Ø±ÙˆØ¹ Ù…Ù†Ø·Ù‚ Ø§ØµÙ„ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡ ---

if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = index_documents_from_disk(embeddings)

st.title("ğŸ¢ Ø³Ø§Ù…Ø§Ù†Ù‡ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ù†Ø´")

with st.sidebar:
    st.subheader("Ù…Ø¯ÛŒØ±ÛŒØª Ø§Ø³Ù†Ø§Ø¯")
    uploaded_files = st.file_uploader("Ø¢Ù¾Ù„ÙˆØ¯ PDF", type="pdf", accept_multiple_files=True)

    if uploaded_files and st.button("ØªØ­Ù„ÛŒÙ„ Ùˆ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ú¯Ø°Ø§Ø±ÛŒ"):
        for uploaded_file in uploaded_files:
            file_bytes = uploaded_file.read()
            file_hash = hashlib.sha256(file_bytes).hexdigest()[:16]
            base_name = f"{file_hash}_{uploaded_file.name.replace(' ', '')}"
            ocr_path = OCR_DIR / f"{base_name}.txt"

            if not ocr_path.exists():
                with st.spinner(f"ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ OCR: {uploaded_file.name}"):
                    images = convert_from_bytes(file_bytes, dpi=200)
                    with ThreadPoolExecutor(max_workers=2) as ex:
                        results = list(ex.map(process_single_page, enumerate(images)))
                    results.sort(key=lambda x: x[0])
                    full_text = "\n\n".join([r[1] for r in results])
                    with open(ocr_path, "w", encoding="utf-8") as f: f.write(full_text)
                    with open(METADATA_DIR / f"{base_name}.json", "w", encoding="utf-8") as f:
                        json.dump({"original_filename": uploaded_file.name, "ocr_text_path": str(ocr_path)}, f)
        
        st.session_state.vectorstore = index_documents_from_disk(embeddings)
        st.rerun()

    if st.button("Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ú¯ÙØªÚ¯Ùˆ"):
        st.session_state.messages = []
        st.rerun()

if "messages" not in st.session_state: st.session_state.messages = []
for m in st.session_state.messages:
    with st.chat_message(m["role"]): st.markdown(m["content"])

if prompt := st.chat_input("Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù¾Ø±Ø³ÛŒØ¯..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"): st.markdown(prompt)

    with st.chat_message("assistant"):
        if st.session_state.vectorstore is None:
            st.session_state.vectorstore = index_documents_from_disk(embeddings)

        if st.session_state.vectorstore:
            placeholder = st.empty()
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªØ§Ø¨Ø¹ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± vector_manager (Ø¨Ø§ Ø­ÙØ¸ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù‚Ø¨Ù„ÛŒ k=20)
            raw_docs = st.session_state.vectorstore.similarity_search(prompt, k=20)
            final_docs = apply_reranking(prompt, raw_docs)

            context = "\n\n".join(d.page_content for d in final_docs)
            chain = (
                {"context": lambda _: context, "question": RunnablePassthrough()}
                | prompt_template | llm_engine | StrOutputParser()
            )

            full_res = ""
            for chunk in chain.stream(prompt):
                full_res += chunk
                placeholder.markdown(full_res + "â–Œ")
            placeholder.markdown(full_res)
            st.session_state.messages.append({"role": "assistant", "content": full_res})
        else:
            st.error("âŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø®Ø§Ù„ÛŒ Ø§Ø³Øª.")

if torch.cuda.is_available(): torch.cuda.empty_cache()