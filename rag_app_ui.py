import streamlit as st
import torch
import re
import pytesseract
import gc
from concurrent.futures import ThreadPoolExecutor
from pdf2image import convert_from_bytes
from setup_models import setup_llm_and_embeddings
from langchain_community.vectorstores import FAISS
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser 
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter


# ==========================
# Ù…Ø³ÛŒØ±Ù‡Ø§ Ùˆ Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ OCR Ùˆ metadata
# ==========================
from pathlib import Path
import datetime
import json

DATA_DIR = Path("data")
OCR_DIR = DATA_DIR / "ocr_texts"
METADATA_DIR = DATA_DIR / "metadata"

OCR_DIR.mkdir(parents=True, exist_ok=True)
METADATA_DIR.mkdir(parents=True, exist_ok=True)


st.set_page_config(page_title="Ø³Ø§Ù…Ø§Ù†Ù‡ Ù…Ø±Ú©Ø²ÛŒ ØªØ­Ù„ÛŒÙ„ Ø§Ø³Ù†Ø§Ø¯", layout="wide", page_icon="ğŸ¢")

if "messages" not in st.session_state:
    st.session_state.messages = []
if "retriever" not in st.session_state:
    st.session_state.retriever = None
if "full_raw_text" not in st.session_state:
    st.session_state.full_raw_text = []

embeddings, llm_engine, prompt_template = setup_llm_and_embeddings()








# def load_existing_documents(_embeddings):
#     all_docs = []
#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
#     st.session_state.full_raw_text = []

#     for metadata_file in METADATA_DIR.glob("*.json"):
#         with open(metadata_file, "r", encoding="utf-8") as f:
#             metadata = json.load(f)
#         ocr_path = Path(metadata["ocr_text_path"])
#         if ocr_path.exists():
#             with open(ocr_path, "r", encoding="utf-8") as f:
#                 text = f.read()
#             st.session_state.full_raw_text.append(f"--- {metadata['original_filename']} ---\n{text}")
#             all_docs.append(Document(page_content=text, metadata={"filename": metadata['original_filename']}))

#     if all_docs:
#         vectorstore = FAISS.from_documents(text_splitter.split_documents(all_docs), _embeddings)
#         return vectorstore.as_retriever(search_kwargs={"k": 10})
#     return None





def load_existing_documents(_embeddings):
    all_docs = []
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )

    # Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ state Ù‡Ø§
    st.session_state.full_raw_text = []
    st.session_state.metadata_text = []

    for metadata_file in METADATA_DIR.glob("*.json"):
        with open(metadata_file, "r", encoding="utf-8") as f:
            metadata = json.load(f)

        # ğŸ”‘ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù…ØªØ§Ø¯ÛŒØªØ§ÛŒ Ø´Ù†Ø§Ø³Ù†Ø§Ù…Ù‡â€ŒØ§ÛŒ Ø°Ø®ÛŒØ±Ù‡â€ŒØ´Ø¯Ù‡
        if "book_metadata_text" in metadata:
            st.session_state.metadata_text.extend(metadata["book_metadata_text"])

        ocr_path = Path(metadata["ocr_text_path"])
        if ocr_path.exists():
            with open(ocr_path, "r", encoding="utf-8") as f:
                text = f.read()

            # Ù…ØªÙ† Ú©Ø§Ù…Ù„ Ø¨Ø±Ø§ÛŒ fallback Ùˆ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Â«Ú©Ù„ Ù…ØªÙ†Â»
            st.session_state.full_raw_text.append(
                f"--- {metadata['original_filename']} ---\n{text}"
            )

            # chunking Ø¨Ø±Ø§ÛŒ RAG
            chunks = text_splitter.split_text(text)
            for i, chunk in enumerate(chunks):
                all_docs.append(
                    Document(
                        page_content=chunk,
                        metadata={
                            "filename": metadata["original_filename"],
                            "chunk_id": i
                        }
                    )
                )

    if all_docs:
        vectorstore = FAISS.from_documents(all_docs, _embeddings)
        return vectorstore.as_retriever(search_kwargs={"k": 20})

    return None




# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù‡Ù†Ú¯Ø§Ù… Ø´Ø±ÙˆØ¹ Ø¨Ø±Ù†Ø§Ù…Ù‡
if st.session_state.retriever is None:
    st.session_state.retriever = load_existing_documents(embeddings)











def clean_text_pro(text):
    text = text.replace("ÛŒ", "ÛŒ").replace("Ú©", "Ú©")
    text = re.sub(r'[^\u0600-\u06FF\s\d.,;?!()\-]', ' ', text)
    return " ".join(text.split())

def process_single_page(args):
    idx, image = args
    raw_text = pytesseract.image_to_string(image, lang='fas')
    return idx + 1, clean_text_pro(raw_text)

def process_high_quality(uploaded_files, _embeddings):
    all_docs = []
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    st.session_state.full_raw_text = []
    
    for uploaded_file in uploaded_files:
        with st.spinner(f"Ø¯Ø± Ø­Ø§Ù„ Ù†Ù…Ø§ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ù†Ø¯: {uploaded_file.name}"):
            images = convert_from_bytes(uploaded_file.read(), dpi=200)
            with ThreadPoolExecutor(max_workers=4) as executor:
                results = list(executor.map(process_single_page, enumerate(images)))
            
            results.sort(key=lambda x: x[0])
            for page_num, text in results:
                # Ø°Ø®ÛŒØ±Ù‡ Ù…ØªÙ† Ø®Ø§Ù… Ø¨Ø±Ø§ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ Â«Ú©Ù„ Ù…ØªÙ†Â» Ø¨Ø¯ÙˆÙ† Ø¯Ø®Ø§Ù„Øª Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ
                st.session_state.full_raw_text.append(f"--- ØµÙØ­Ù‡ {page_num} ---\n{text}")
                all_docs.append(Document(page_content=text, metadata={"page": page_num}))
            gc.collect()
    
    vectorstore = FAISS.from_documents(text_splitter.split_documents(all_docs), _embeddings)
    return vectorstore.as_retriever(search_kwargs={"k": 20})








# ==========================
# Ù†Ø³Ø®Ù‡ Ø¬Ø¯ÛŒØ¯ ØªØ§Ø¨Ø¹ OCR Ùˆ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
# ==========================
def process_high_quality_v2(uploaded_files, _embeddings):
    all_docs = []
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    st.session_state.full_raw_text = []

    def process_single_page_inner(args):
        idx, image = args
        raw_text = pytesseract.image_to_string(image, lang='fas')
        text = raw_text.replace("ÛŒ", "ÛŒ").replace("Ú©", "Ú©")
        import re
        return idx + 1, " ".join(re.sub(r'[^\u0600-\u06FF\s\d.,;?!()\-]', ' ', text).split())

    for uploaded_file in uploaded_files:
        original_name = uploaded_file.name
        today = datetime.date.today().strftime("%Y%m%d")
        base_filename = f"{today}{original_name.replace(' ', '')}"
        ocr_path = OCR_DIR / f"{base_filename}.txt"
        metadata_path = METADATA_DIR / f"{base_filename}.json"
        # ğŸ”‘ Ú©Ù„Ù…Ø§Øª Ø±Ø§Ù‡Ù†Ù…Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø´Ù†Ø§Ø³Ù†Ø§Ù…Ù‡â€ŒØ§ÛŒ
        META_HINTS = ["Ø´Ø§Ø¨Ú©", "ISBN", "Ø§Ù†ØªØ´Ø§Ø±Ø§Øª", "Ù†Ø§Ø´Ø±", "Ú†Ø§Ù¾", "Ù‚ÛŒÙ…Øª", "Ø±ÛŒØ§Ù„", "ØªÙˆÙ…Ø§Ù†"]
        metadata_texts = []

        # Ø§Ú¯Ø± ÙØ§ÛŒÙ„ Ù‚Ø¨Ù„Ø§Ù‹ OCR Ø´Ø¯Ù‡ØŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
        if metadata_path.exists() and ocr_path.exists():
            st.info(f"ÙØ§ÛŒÙ„ {original_name} Ø§Ø² Ù‚Ø¨Ù„ OCR Ø´Ø¯Ù‡ Ø§Ø³Øª. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù†Ø³Ø®Ù‡ Ù…ÙˆØ¬ÙˆØ¯.")
            with open(ocr_path, "r", encoding="utf-8") as f:
                text = f.read()
            st.session_state.full_raw_text.append(f"--- {original_name} ---\n{text}")
            all_docs.append(Document(page_content=text, metadata={"filename": original_name}))
            continue

        # OCR Ø¬Ø¯ÛŒØ¯
        with st.spinner(f"Ø¯Ø± Ø­Ø§Ù„ Ù†Ù…Ø§ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ù†Ø¯: {original_name}"):
            images = convert_from_bytes(uploaded_file.read(), dpi=200)
            all_text_pages = []

            from concurrent.futures import ThreadPoolExecutor
            import gc

            with ThreadPoolExecutor(max_workers=4) as executor:
                results = list(executor.map(process_single_page_inner, enumerate(images)))

            results.sort(key=lambda x: x[0])
            for page_num, page_text in results:
                st.session_state.full_raw_text.append(f"--- ØµÙØ­Ù‡ {page_num} ---\n{page_text}")
                all_text_pages.append(page_text)
                all_docs.append(Document(page_content=page_text, metadata={"filename": original_name, "page": page_num}))
                # ğŸ” Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø´Ù†Ø§Ø³Ù†Ø§Ù…Ù‡â€ŒØ§ÛŒ
                if any(k in page_text for k in META_HINTS):
                    metadata_texts.append(page_text)

            # Ø°Ø®ÛŒØ±Ù‡ Ù…ØªÙ† OCR
            with open(ocr_path, "w", encoding="utf-8") as f:
                f.write("\n\n".join(all_text_pages))

            # Ø°Ø®ÛŒØ±Ù‡ metadata
            metadata = {
                "original_filename": original_name,
                "ocr_text_path": str(ocr_path),
                "upload_date": today,
                "num_pages": len(all_text_pages),
                "book_metadata_text": metadata_texts
            }
            
            with open(metadata_path, "w", encoding="utf-8") as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            # ğŸ”‘ Ø°Ø®ÛŒØ±Ù‡ Ù…ØªØ§Ø¯ÛŒØªØ§ÛŒ Ø´Ù†Ø§Ø³Ù†Ø§Ù…Ù‡â€ŒØ§ÛŒ Ø¯Ø± session_state
            if "metadata_text" not in st.session_state:
                st.session_state.metadata_text = []

            st.session_state.metadata_text.extend(metadata_texts)

            gc.collect()

    # Ø³Ø§Ø®Øª vectorstore
    vectorstore = FAISS.from_documents(text_splitter.split_documents(all_docs), _embeddings)
    return vectorstore.as_retriever(search_kwargs={"k": 20})








# --- UI ---
st.title("ğŸ¢ Ø³Ø§Ù…Ø§Ù†Ù‡ Ù…Ø±Ú©Ø²ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ù†Ø´ Ùˆ ØªØ­Ù„ÛŒÙ„ Ø§Ø³Ù†Ø§Ø¯")
st.caption("Ù†Ø³Ø®Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Ø¯Ø§ÛŒÙ†Ø§Ù…ÛŒÚ© - ÙˆÙØ§Ø¯Ø§Ø±ÛŒ Ù…Ø·Ù„Ù‚ Ø¨Ù‡ Ù…ØªÙ†")

with st.sidebar:
    st.header("Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø§Ø³Ù†Ø§Ø¯")
    uploaded_files = st.file_uploader("ÙØ§ÛŒÙ„ PDF Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯", type="pdf", accept_multiple_files=True)
    
    
    # if uploaded_files and st.button("Ø´Ø±ÙˆØ¹ ØªØ­Ù„ÛŒÙ„"):
    #     # st.session_state.retriever = process_high_quality(uploaded_files, embeddings)
    #     st.session_state.retriever = process_high_quality_v2(uploaded_files, embeddings)
        
    #     st.success("ÙØ±Ø¢ÛŒÙ†Ø¯ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")
    #     st.rerun()
        
        
        
    if uploaded_files and st.button("Ø´Ø±ÙˆØ¹ ØªØ­Ù„ÛŒÙ„"):
    # OCR Ùˆ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
        process_high_quality_v2(uploaded_files, embeddings)
    # Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ retriever Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù‡Ù…Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ (Ø¬Ø¯ÛŒØ¯ Ùˆ Ù‚Ø¨Ù„ÛŒ)
        st.session_state.retriever = load_existing_documents(embeddings)
        st.success("ÙØ±Ø¢ÛŒÙ†Ø¯ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")
        st.rerun()
        
        
        
        
        
        
        
        
        

for message in st.session_state.messages:
    with st.chat_message(message["role"]): st.markdown(message["content"])

if prompt := st.chat_input("Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù¾Ø±Ø³ÛŒØ¯..."):
    if st.session_state.retriever:
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"): st.markdown(prompt)
        
        with st.chat_message("assistant"):
            placeholder = st.empty()
            
            # Û±. Ù¾Ø§Ø³Ø® Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ù‡ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Â«Ú©Ù„ Ù…ØªÙ†Â» (Ø¨Ø¯ÙˆÙ† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÙˆÙ‡Ù…)
            if any(x in prompt for x in ["Ú©Ù„ Ù…ØªÙ†", "ØªÙ…Ø§Ù… Ù…ØªÙ†", "Ù…ØªÙ† Ú©Ø§Ù…Ù„"]):
                full_res = "### Ù…ØªÙ† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ Ø§Ø² Ú©Ù„ Ø¯Ø§Ú©ÛŒÙˆÙ…Ù†Øª:\n\n" + "\n\n".join(st.session_state.full_raw_text)
                placeholder.markdown(full_res)
            
            # Û². Ù¾Ø§Ø³Ø®Ú¯ÙˆÛŒÛŒ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± RAG
            else:
                full_res = ""
                # ØªØ´Ø®ÛŒØµ Ø³ÙˆØ§Ù„Ø§Øª Ø´Ù†Ø§Ø³Ù†Ø§Ù…Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÙˆÙ„ÙˆÛŒØªâ€ŒØ¨Ù†Ø¯ÛŒ ØµÙØ­Ø§Øª Ø§ÙˆÙ„                                                   
                is_meta = any(k in prompt for k in ["Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡", "Ù‚ÛŒÙ…Øª", "Ù†Ø§Ø´Ø±", "ØªÛŒØ±Ø§Ú˜", "Ú†Ø§Ù¾", "Ø´Ø§Ø¨Ú©", "ISBN"])

                if is_meta and "metadata_text" in st.session_state and st.session_state.metadata_text:
                    # ğŸ”‘ Ù¾Ø§Ø³Ø® Ø¯Ù‚ÛŒÙ‚ ÙÙ‚Ø· Ø§Ø² Ù…ØªØ§Ø¯ÛŒØªØ§ÛŒ Ø´Ù†Ø§Ø³Ù†Ø§Ù…Ù‡â€ŒØ§ÛŒ
                    context = "\n\n".join(st.session_state.metadata_text)
                else:
                    # ğŸ” RAG Ù…Ø¹Ù…ÙˆÙ„ÛŒ
                    docs = st.session_state.retriever.invoke(prompt)
                    context = "\n\n".join([d.page_content for d in docs])

                    # ğŸ” fallback Ø§Ú¯Ø± Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø¶Ø¹ÛŒÙ Ø¨ÙˆØ¯
                    if len(context.strip()) < 800:
                        context = "\n".join(st.session_state.full_raw_text)
                    
                    

                # if context.strip():
                #     try:
                #         chain = ({"context": lambda x: context, "question": RunnablePassthrough()} | prompt_template | llm_engine | StrOutputParser())
                #         for chunk in chain.stream(prompt):
                #             full_res += chunk
                #             placeholder.markdown(full_res + "â–Œ")
                #     except Exception:
                #         full_res = "âš ï¸ Ø®Ø·Ø§ÛŒ ÙÙ†ÛŒ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Ù…ØªÙ†. Ù„Ø·ÙØ§Ù‹ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯."
                # else:
                #     full_res = "Ø¯Ø± Ø§Ø³Ù†Ø§Ø¯ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ø§ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹ ÛŒØ§ÙØª Ù†Ø´Ø¯."



                if context.strip():
                    try:
                        chain = ({"context": lambda x: context, "question": RunnablePassthrough()} | prompt_template | llm_engine | StrOutputParser())
                        for chunk in chain.stream(prompt):
                            full_res += chunk
                            placeholder.markdown(full_res + "â–Œ")
                    except Exception:
                        full_res = "âš ï¸ Ø®Ø·Ø§ÛŒ ÙÙ†ÛŒ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Ù…ØªÙ†. Ù„Ø·ÙØ§Ù‹ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯."
                else:
                    full_res = "â„¹ï¸ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ø¯Ø± Ø§Ø³Ù†Ø§Ø¯ Ù…ÙˆØ¬ÙˆØ¯ ÛŒØ§ÙØª Ù†Ø´Ø¯."
    
    
    
    
                
                placeholder.markdown(full_res)
            
            st.session_state.messages.append({"role": "assistant", "content": full_res})

if torch.cuda.is_available(): torch.cuda.empty_cache()




