import streamlit as st
import torch
import re
import pytesseract
from pdf2image import convert_from_bytes
from setup_models import setup_llm_and_embeddings, format_docs
from langchain_community.vectorstores import FAISS
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser 
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter

st.set_page_config(page_title="ØªØ­Ù„ÛŒÙ„Ú¯Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø­Ø±Ú©Øª", layout="wide")

# Û±. Ù…Ø¯ÛŒØ±ÛŒØª ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ú†Øª (Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø­Ø°Ù Ø³ÙˆØ§Ù„Ø§Øª Ù‚Ø¨Ù„ÛŒ)
if "messages" not in st.session_state:
    st.session_state.messages = []

if "retriever" not in st.session_state:
    st.session_state.retriever = None

try:
    embeddings, llm_engine, prompt_template = setup_llm_and_embeddings()
except Exception as e:
    st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù„ÙˆØ¯ Ù…Ø¯Ù„: {e}")
    st.stop()

def clean_text_pro(text):
    text = text.replace("ÛŒ", "ÛŒ").replace("Ú©", "Ú©")
    f_digits, e_digits = "Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹", "0123456789"
    text = text.translate(str.maketrans(f_digits, e_digits))
    text = re.sub(r'[^\u0600-\u06FF\s\d.,;?!()\-]', ' ', text)
    return " ".join(text.split())

def process_high_quality(uploaded_files, _embeddings):
    all_docs = []
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=400)
    
    for uploaded_file in uploaded_files:
        with st.spinner(f"Ø¯Ø± Ø­Ø§Ù„ Ø§Ø³Ú©Ù† Ø¯Ù‚ÛŒÙ‚ Ú©ØªØ§Ø¨ (DPI 300)..."):
            images = convert_from_bytes(uploaded_file.read(), dpi=300)
            for i, image in enumerate(images):
                raw_text = pytesseract.image_to_string(image, lang='fas')
                cleaned = clean_text_pro(raw_text)
                
                # ØªÙ‚ÙˆÛŒØª Ø´Ù†Ø§Ø³Ù†Ø§Ù…Ù‡: Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø±Ø§ Ø¨Ù‡ Ù…ØªØ§Ø¯ÛŒØªØ§ÛŒ ØµÙØ­Ø§Øª Ø§ÙˆÙ„ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
                prefix = ""
                if i < 3:
                    prefix = "[Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø´Ù†Ø§Ø³Ù†Ø§Ù…Ù‡: Ù†Ø§Ù… Ú©ØªØ§Ø¨ØŒ Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡ØŒ Ú†Ø§Ù¾ØŒ Ù‚ÛŒÙ…ØªØŒ ØªÛŒØ±Ø§Ú˜ØŒ Ù†Ø§Ø´Ø±] "
                
                if len(cleaned) > 25:
                    all_docs.append(Document(
                        page_content=prefix + cleaned, 
                        metadata={"page": i+1}
                    ))
    
    vectorstore = FAISS.from_documents(text_splitter.split_documents(all_docs), _embeddings)
    # k=15 Ø¨Ø±Ø§ÛŒ Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§ Ø¯Ø± Ù…ÙØ§Ù‡ÛŒÙ…
    return vectorstore.as_retriever(search_kwargs={"k": 15})

# --- Ù¾Ù†Ù„ Ú©Ù†Ø§Ø±ÛŒ ---
with st.sidebar:
    st.header("ğŸ“‚ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©ØªØ§Ø¨")
    files = st.file_uploader("ÙØ§ÛŒÙ„ PDF", type="pdf", accept_multiple_files=True)
    if st.button("ğŸš€ Ø´Ø±ÙˆØ¹ ØªØ­Ù„ÛŒÙ„"):
        if files:
            st.session_state.retriever = process_high_quality(files, embeddings)
            st.success("ØªØ­Ù„ÛŒÙ„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")

# Û². Ù†Ù…Ø§ÛŒØ´ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ú¯ÙØªÚ¯Ùˆ (Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù…Ø§Ù†Ø¹ Ù¾Ø§Ú© Ø´Ø¯Ù† Ø³ÙˆØ§Ù„Ø§Øª Ù‚Ø¨Ù„ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯)
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Û³. Ø¯Ø±ÛŒØ§ÙØª Ø³ÙˆØ§Ù„ Ø¬Ø¯ÛŒØ¯
if prompt := st.chat_input("Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù¾Ø±Ø³ÛŒØ¯..."):
    # Ø°Ø®ÛŒØ±Ù‡ Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø± Ø¯Ø± ØªØ§Ø±ÛŒØ®Ú†Ù‡
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    if st.session_state.retriever:
        with st.chat_message("assistant"):
            with st.spinner("Ø¯Ø± Ø­Ø§Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù¾Ø§Ø³Ø®..."):
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                docs = st.session_state.retriever.invoke(prompt)
                context = format_docs(docs)
                
                chain = (
                    {"context": lambda x: context, "question": RunnablePassthrough()} 
                    | prompt_template | llm_engine | StrOutputParser()
                )
                
                response = chain.invoke(prompt)
                pages = ", ".join(set([str(d.metadata['page']) for d in docs]))
                full_res = f"{response}\n\n*ğŸ“ Ù…Ù†Ø§Ø¨Ø¹:* ØµÙØ­Ø§Øª {pages}"
                
                st.markdown(full_res)
                # Ø°Ø®ÛŒØ±Ù‡ Ù¾Ø§Ø³Ø® Ø³ÛŒØ³ØªÙ… Ø¯Ø± ØªØ§Ø±ÛŒØ®Ú†Ù‡
                st.session_state.messages.append({"role": "assistant", "content": full_res})

if torch.cuda.is_available():
    torch.cuda.empty_cache()
