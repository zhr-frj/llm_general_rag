import streamlit as st
import torch
import re
import pytesseract
import gc
from concurrent.futures import ThreadPoolExecutor
from pdf2image import convert_from_bytes
from setup_models import setup_llm_and_embeddings
from langchain_community.vectorstores import FAISS
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser 
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter

st.set_page_config(page_title="Ø³Ø§Ù…Ø§Ù†Ù‡ Ù…Ø±Ú©Ø²ÛŒ ØªØ­Ù„ÛŒÙ„ Ø§Ø³Ù†Ø§Ø¯", layout="wide", page_icon="ğŸ¢")

if "messages" not in st.session_state:
    st.session_state.messages = []
if "retriever" not in st.session_state:
    st.session_state.retriever = None
if "full_raw_text" not in st.session_state:
    st.session_state.full_raw_text = []

embeddings, llm_engine, prompt_template = setup_llm_and_embeddings()

def clean_text_pro(text):
    text = text.replace("ÛŒ", "ÛŒ").replace("Ú©", "Ú©")
    text = re.sub(r'[^\u0600-\u06FF\s\d.,;?!()\-]', ' ', text)
    return " ".join(text.split())

def process_single_page(args):
    idx, image = args
    raw_text = pytesseract.image_to_string(image, lang='fas')
    return idx + 1, clean_text_pro(raw_text)

def process_high_quality(uploaded_files, _embeddings):
    all_docs = []
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    st.session_state.full_raw_text = []
    
    for uploaded_file in uploaded_files:
        with st.spinner(f"Ø¯Ø± Ø­Ø§Ù„ Ù†Ù…Ø§ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ù†Ø¯: {uploaded_file.name}"):
            images = convert_from_bytes(uploaded_file.read(), dpi=200)
            with ThreadPoolExecutor(max_workers=4) as executor:
                results = list(executor.map(process_single_page, enumerate(images)))
            
            results.sort(key=lambda x: x[0])
            for page_num, text in results:
                # Ø°Ø®ÛŒØ±Ù‡ Ù…ØªÙ† Ø®Ø§Ù… Ø¨Ø±Ø§ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ Â«Ú©Ù„ Ù…ØªÙ†Â» Ø¨Ø¯ÙˆÙ† Ø¯Ø®Ø§Ù„Øª Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ
                st.session_state.full_raw_text.append(f"--- ØµÙØ­Ù‡ {page_num} ---\n{text}")
                all_docs.append(Document(page_content=text, metadata={"page": page_num}))
            gc.collect()
    
    vectorstore = FAISS.from_documents(text_splitter.split_documents(all_docs), _embeddings)
    return vectorstore.as_retriever(search_kwargs={"k": 10})

# --- UI ---
st.title("ğŸ¢ Ø³Ø§Ù…Ø§Ù†Ù‡ Ù…Ø±Ú©Ø²ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ù†Ø´ Ùˆ ØªØ­Ù„ÛŒÙ„ Ø§Ø³Ù†Ø§Ø¯")
st.caption("Ù†Ø³Ø®Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Ø¯Ø§ÛŒÙ†Ø§Ù…ÛŒÚ© - ÙˆÙØ§Ø¯Ø§Ø±ÛŒ Ù…Ø·Ù„Ù‚ Ø¨Ù‡ Ù…ØªÙ†")

with st.sidebar:
    st.header("Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø§Ø³Ù†Ø§Ø¯")
    uploaded_files = st.file_uploader("ÙØ§ÛŒÙ„ PDF Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯", type="pdf", accept_multiple_files=True)
    if uploaded_files and st.button("Ø´Ø±ÙˆØ¹ ØªØ­Ù„ÛŒÙ„"):
        st.session_state.retriever = process_high_quality(uploaded_files, embeddings)
        st.success("ÙØ±Ø¢ÛŒÙ†Ø¯ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")
        st.rerun()

for message in st.session_state.messages:
    with st.chat_message(message["role"]): st.markdown(message["content"])

if prompt := st.chat_input("Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù¾Ø±Ø³ÛŒØ¯..."):
    if st.session_state.retriever:
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"): st.markdown(prompt)
        
        with st.chat_message("assistant"):
            placeholder = st.empty()
            
            # Û±. Ù¾Ø§Ø³Ø® Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ù‡ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Â«Ú©Ù„ Ù…ØªÙ†Â» (Ø¨Ø¯ÙˆÙ† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÙˆÙ‡Ù…)
            if any(x in prompt for x in ["Ú©Ù„ Ù…ØªÙ†", "ØªÙ…Ø§Ù… Ù…ØªÙ†", "Ù…ØªÙ† Ú©Ø§Ù…Ù„"]):
                full_res = "### Ù…ØªÙ† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ Ø§Ø² Ú©Ù„ Ø¯Ø§Ú©ÛŒÙˆÙ…Ù†Øª:\n\n" + "\n\n".join(st.session_state.full_raw_text)
                placeholder.markdown(full_res)
            
            # Û². Ù¾Ø§Ø³Ø®Ú¯ÙˆÛŒÛŒ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± RAG
            else:
                full_res = ""
                # ØªØ´Ø®ÛŒØµ Ø³ÙˆØ§Ù„Ø§Øª Ø´Ù†Ø§Ø³Ù†Ø§Ù…Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÙˆÙ„ÙˆÛŒØªâ€ŒØ¨Ù†Ø¯ÛŒ ØµÙØ­Ø§Øª Ø§ÙˆÙ„
                is_meta = any(k in prompt for k in ["Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡", "Ù‚ÛŒÙ…Øª", "Ù†Ø§Ø´Ø±", "ØªÛŒØ±Ø§Ú˜", "Ú†Ø§Ù¾", "Ù…Ø´Ø®ØµØ§Øª"])
                
                if is_meta:
                    # Ø¨Ø±Ø§ÛŒ Ù…Ø´Ø®ØµØ§ØªØŒ ØµÙØ­Ø§Øª Ø§Ø¨ØªØ¯Ø§ÛŒÛŒ Ø±Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø§ÙˆÙ„ÙˆÛŒØª Ø§ÙˆÙ„ Ø¨ÙØ±Ø³Øª
                    context = "\n".join(st.session_state.full_raw_text[:5])
                else:
                    # Ø¨Ø±Ø§ÛŒ Ø³Ø§ÛŒØ± Ø³ÙˆØ§Ù„Ø§ØªØŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø§Ù†Ø¬Ø§Ù… Ø¨Ø¯Ù‡
                    docs = st.session_state.retriever.invoke(prompt)
                    context = "\n\n".join([d.page_content for d in docs])

                if context.strip():
                    try:
                        chain = ({"context": lambda x: context, "question": RunnablePassthrough()} | prompt_template | llm_engine | StrOutputParser())
                        for chunk in chain.stream(prompt):
                            full_res += chunk
                            placeholder.markdown(full_res + "â–Œ")
                    except Exception:
                        full_res = "âš ï¸ Ø®Ø·Ø§ÛŒ ÙÙ†ÛŒ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Ù…ØªÙ†. Ù„Ø·ÙØ§Ù‹ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯."
                else:
                    full_res = "Ø¯Ø± Ø§Ø³Ù†Ø§Ø¯ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ø§ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹ ÛŒØ§ÙØª Ù†Ø´Ø¯."
                
                placeholder.markdown(full_res)
            
            st.session_state.messages.append({"role": "assistant", "content": full_res})

if torch.cuda.is_available(): torch.cuda.empty_cache()
