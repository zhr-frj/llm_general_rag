# # import streamlit as st
# # import torch, gc, json, hashlib, time, uuid, faiss, psutil, os, base64, re
# # from pathlib import Path
# # from pdf2image import convert_from_bytes
# # import pytesseract
# # from concurrent.futures import ThreadPoolExecutor
# # from vector_manager import load_vectorstore_on_gpu
# # from setup_models import setup_llm_and_embeddings
# # from langchain_community.vectorstores import FAISS
# # from langchain_text_splitters import RecursiveCharacterTextSplitter
# # from langchain_core.documents import Document

# # # --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡ ØµÙØ­Ù‡ ---
# # st.set_page_config(page_title="Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø³Ø§Ø²Ù…Ø§Ù†ÛŒ", layout="wide")


# # def get_base64_font(font_path):
# #     if os.path.exists(font_path):
# #         with open(font_path, "rb") as f:
# #             return base64.b64encode(f.read()).decode()
# #     return ""


# # def apply_custom_styles():
# #     icon_css_path = "icons/bootstrap-icons.css"
# #     font_path_woff2 = "icons/fonts/bootstrap-icons.woff2"
# #     vazir_font_path = "icons/fonts/Vazirmatn.woff2"
# #     font_base64 = get_base64_font(font_path_woff2)
# #     vazir_base64 = get_base64_font(vazir_font_path)
# #     css_content = ""
# #     if os.path.exists(icon_css_path):
# #         with open(icon_css_path, "r") as f:
# #             css_content = f.read()

# #     st.markdown(
# #         f"""
# #     <style>
# #     @font-face {{ font-family: 'bootstrap-icons'; src: url(data:font/woff2;base64,{font_base64}) format('woff2'); }}
# #     @font-face {{ font-family: 'Vazirmatn'; src: url(data:font/woff2;base64,{vazir_base64}) format('woff2'); }}
# #     {css_content}

# #     /* ØªÙ†Ø¸ÛŒÙ… Ú©Ù„ÛŒ Ø¬Ù‡Øª ØµÙØ­Ù‡ Ùˆ ÙÙˆÙ†Øª */
# #     html, body, [data-testid="stAppViewContainer"], [data-testid="stHeader"] {{
# #         font-family: 'Vazirmatn', sans-serif !important;
# #         direction: rtl !important;
# #         text-align: right !important;
# #     }}

# #     /* Ø§ØµÙ„Ø§Ø­ Ø±ÙØªØ§Ø± Ø³Ø§ÛŒØ¯Ø¨Ø§Ø± Ø¯Ø± Ø­Ø§Ù„Øª RTL - Ø­Ø°Ù Ù¾ÙˆØ²ÛŒØ´Ù† ÙÛŒÚ©Ø³ Ø¯Ø³ØªÛŒ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø¨Ø§Ú¯ Ø¨ØµØ±ÛŒ */
# #     [data-testid="stSidebar"] {{
# #         background-color: #111827 !important;
# #         min-width: 320px !important;
# #         max-width: 320px !important;
# #         direction: rtl !important;
# #     }}

# #     /* ÙÛŒÚ©Ø³ Ú©Ø±Ø¯Ù† Ø¯Ú©Ù…Ù‡ ÙÙ„Ø´ Ø¨Ø§Ø²Ú©Ù†Ù†Ø¯Ù‡ Ø³Ø§ÛŒØ¯Ø¨Ø§Ø± Ø¯Ø± Ø³Ù…Øª Ø±Ø§Ø³Øª */
# #     [data-testid="stSidebarCollapsedControl"] {{
# #         right: 0 !important;
# #         left: auto !important;
# #         background-color: #111827 !important;
# #         display: flex !important;
# #         justify-content: center !important;
# #         border-radius: 5px 0 0 5px !important;
# #     }}

# #     /* Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ Ú©Ù„ ÙØ¶Ø§ Ø±Ø§ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ Ùˆ Ø²ÛŒØ± Ø³Ø§ÛŒØ¯Ø¨Ø§Ø± Ù†Ù…ÛŒâ€ŒØ±ÙˆØ¯ */
# #     [data-testid="stMainViewContainer"] {{
# #         width: 100% !important;
# #     }}

# #     .stChatMessage {{ direction: rtl !important; text-align: right !important; }}
# #     .monitor-card {{ background: #064e3b; color: #34d399; padding: 12px; border-radius: 10px; margin-bottom: 8px; border-right: 5px solid #10b981; }}
# #     </style>
# #     """,
# #         unsafe_allow_html=True,
# #     )


# # def clean_ocr_text(text):
# #     text = re.sub(r"[Â°Ï„Âµ~|â€”_]{2,}", "", text)
# #     text = re.sub(r"\s+", " ", text)
# #     return text.strip()


# # apply_custom_styles()

# # DATA_DIR = Path("data")
# # INDEX_PATH = DATA_DIR / "vectorstore"
# # METADATA_DIR = DATA_DIR / "metadata"
# # for p in [METADATA_DIR, INDEX_PATH]:
# #     p.mkdir(parents=True, exist_ok=True)

# # with st.spinner("â³ Ø¯Ø± Ø­Ø§Ù„ Ø¨ÛŒØ¯Ø§Ø± Ú©Ø±Ø¯Ù† Ù…Ø¯Ù„â€ŒÙ‡Ø§..."):
# #     embeddings, llm_engine, prompt_template, (rerank_m, rerank_t) = (
# #         setup_llm_and_embeddings()
# #     )

# # if "vectorstore" not in st.session_state:
# #     st.session_state.vectorstore = load_vectorstore_on_gpu(
# #         str(INDEX_PATH), embeddings, 1
# #     )

# # if "full_texts" not in st.session_state:
# #     st.session_state.full_texts = {}
# #     for meta_file in METADATA_DIR.glob("*.json"):
# #         with open(meta_file, "r") as f:
# #             data = json.load(f)
# #             if "full_content" in data:
# #                 st.session_state.full_texts[data["name"]] = data["full_content"]

# # # --- Ù…Ø¯ÛŒØ±ÛŒØª Ø³Ø§ÛŒØ¯Ø¨Ø§Ø± ---
# # with st.sidebar:
# #     # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªÚ¯ h3 Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªØ¯Ø§Ø®Ù„ Ù†Ù…Ø§ÛŒØ´ Ø¢ÛŒÚ©ÙˆÙ†
# #     st.markdown(
# #         '<h3 style="color: white; direction: rtl; text-align: right;"><i class="bi bi-cpu-fill"></i> Ù¾Ù†Ù„ Ù…Ø¯ÛŒØ±ÛŒØª</h3>',
# #         unsafe_allow_html=True,
# #     )

# #     with st.expander("ğŸ“Š Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø±", expanded=False):
# #         for i in range(torch.cuda.device_count()):
# #             used = torch.cuda.memory_reserved(i) / 1024**3
# #             st.markdown(
# #                 f'<div class="monitor-card"><i class="bi bi-gpu-card"></i> <b>Ú©Ø§Ø±Øª Ú¯Ø±Ø§ÙÛŒÚ© {i}</b><br>Ù…ØµØ±Ù: {used:.1f} GB</div>',
# #                 unsafe_allow_html=True,
# #             )

# #         st.markdown(
# #             f'<div class="monitor-card" style="background:#1e293b;"><i class="bi bi-cpu"></i> <b>Ø±Ù… Ø³ÛŒØ³ØªÙ…: {psutil.virtual_memory().percent}%</b></div>',
# #             unsafe_allow_html=True,
# #         )

# #     st.divider()
# #     files = st.file_uploader("Ø¢Ù¾Ù„ÙˆØ¯ PDF", type="pdf", accept_multiple_files=True)

# #     if files and st.button("ğŸª„ Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´"):
# #         new_docs = []
# #         with st.status("ğŸš€ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡ÙˆØ´Ù…Ù†Ø¯...", expanded=True) as status:
# #             for f in files:
# #                 f_bytes = f.read()
# #                 f_hash = hashlib.md5(f_bytes).hexdigest()
# #                 meta_path = METADATA_DIR / f"{f_hash}.json"

# #                 if meta_path.exists():
# #                     st.toast(f"âœ… ÙØ§ÛŒÙ„ {f.name} Ù‚Ø¨Ù„Ø§Ù‹ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù…ÙˆØ¬ÙˆØ¯ Ø¨ÙˆØ¯.", icon="ğŸ’¾")
# #                     with open(meta_path, "r") as m:
# #                         combined_text = json.load(m)["full_content"]
# #                 else:
# #                     status.write(f"ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø¬Ø¯ÛŒØ¯: {f.name}")
# #                     imgs = convert_from_bytes(f_bytes, dpi=150)
# #                     with ThreadPoolExecutor(max_workers=4) as exe:
# #                         texts = list(
# #                             exe.map(
# #                                 lambda img: pytesseract.image_to_string(
# #                                     img, lang="fas+eng"
# #                                 ),
# #                                 imgs,
# #                             )
# #                         )

# #                     combined_text = clean_ocr_text("\n\n".join(texts))
# #                     with open(meta_path, "w") as m:
# #                         json.dump({"name": f.name, "full_content": combined_text}, m)
# #                     st.toast(f"âœ¨ ÙØ§ÛŒÙ„ {f.name} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯.", icon="âœ…")

# #                 st.session_state.full_texts[f.name] = combined_text
# #                 new_docs.append(
# #                     Document(page_content=combined_text, metadata={"source": f.name})
# #                 )

# #             if new_docs:
# #                 splits = RecursiveCharacterTextSplitter(
# #                     chunk_size=800, chunk_overlap=200
# #                 ).split_documents(new_docs)
# #                 vs = FAISS.from_documents(splits, embeddings)
# #                 vs.index = faiss.index_gpu_to_cpu(vs.index)
# #                 vs.save_local(str(INDEX_PATH))
# #                 st.session_state.vectorstore = load_vectorstore_on_gpu(
# #                     str(INDEX_PATH), embeddings, 1
# #                 )
# #                 status.update(label="âœ… Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø´Ø¯!", state="complete")
# #                 st.rerun()

# # # --- Ø¨Ø®Ø´ Ø§ØµÙ„ÛŒ Ú¯ÙØªÚ¯Ùˆ ---
# # st.title("ğŸ¢ Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù…Ø¯ÛŒØ±ÛŒØª Ø¯Ø§Ù†Ø´")
# # if "messages" not in st.session_state:
# #     st.session_state.messages = []

# # for m in st.session_state.messages:
# #     with st.chat_message(m["role"], avatar="ğŸ‘¤" if m["role"] == "user" else "ğŸ¤–"):
# #         st.markdown(
# #             f'<div style="text-align: right; direction: rtl;">{m["content"]}</div>',
# #             unsafe_allow_html=True,
# #         )

# # if prompt := st.chat_input("Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù¾Ø±Ø³ÛŒØ¯..."):
# #     st.session_state.messages.append({"role": "user", "content": prompt})
# #     with st.chat_message("user", avatar="ğŸ‘¤"):
# #         st.markdown(
# #             f'<div style="text-align: right; direction: rtl;">{prompt}</div>',
# #             unsafe_allow_html=True,
# #         )

# #     with st.chat_message("assistant", avatar="ğŸ¤–"):
# #         if st.session_state.vectorstore:
# #             with st.spinner("ğŸ” Ø¯Ø± Ø­Ø§Ù„ ØªØ­Ù„ÛŒÙ„..."):
# #                 docs = st.session_state.vectorstore.similarity_search(prompt, k=15)
# #                 pairs = [[prompt, d.page_content] for d in docs]
# #                 inputs = rerank_t(
# #                     pairs,
# #                     padding=True,
# #                     truncation=True,
# #                     return_tensors="pt",
# #                     max_length=512,
# #                 ).to("cuda:1")

# #                 with torch.no_grad():
# #                     scores = rerank_m(**inputs).logits.view(-1).float()

# #                 context = "\n\n".join(
# #                     [
# #                         docs[i].page_content
# #                         for i in torch.argsort(scores, descending=True)[:8]
# #                     ]
# #                 )
# #                 chain = prompt_template | llm_engine
# #                 response = chain.invoke({"context": context, "question": prompt})

# #                 # ØªÙ…ÛŒØ²Ú©Ø§Ø±ÛŒ Ø®Ø±ÙˆØ¬ÛŒ
# #                 ans = (
# #                     response.split("Ù¾Ø§Ø³Ø® Ø¬Ø§Ù…Ø¹ Ùˆ ØªØ´Ø±ÛŒØ­ÛŒ:")[-1].strip()
# #                     if "Ù¾Ø§Ø³Ø® Ø¬Ø§Ù…Ø¹ Ùˆ ØªØ´Ø±ÛŒØ­ÛŒ:" in response
# #                     else response
# #                 )
# #                 ans = re.sub(r"\*+", "", ans).strip()

# #                 st.markdown(
# #                     f'<div style="text-align: right; direction: rtl;">{ans}</div>',
# #                     unsafe_allow_html=True,
# #                 )
# #                 st.session_state.messages.append({"role": "assistant", "content": ans})

# #     torch.cuda.empty_cache()
# #     gc.collect()


# ##rag_app_ui.py


# import streamlit as st
# import torch, gc, json, hashlib, time, uuid, faiss, psutil, os, base64, re
# from pathlib import Path
# from pdf2image import convert_from_bytes
# import pytesseract
# from concurrent.futures import ThreadPoolExecutor
# from vector_manager import load_vectorstore_on_gpu
# from setup_models import setup_llm_and_embeddings
# from langchain_community.vectorstores import FAISS
# from langchain_text_splitters import RecursiveCharacterTextSplitter
# from langchain_core.documents import Document

# # --- ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† ÙØ§ÛŒÙ„ Ø§Ø³ØªØ§ÛŒÙ„ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ ---
# from style import apply_custom_styles

# # --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡ ØµÙØ­Ù‡ ---
# st.set_page_config(page_title="Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø³Ø§Ø²Ù…Ø§Ù†ÛŒ", layout="wide")

# # --- Ø§Ø¹Ù…Ø§Ù„ Ø§Ø³ØªØ§ÛŒÙ„â€ŒÙ‡Ø§ ---
# apply_custom_styles()


# def clean_ocr_text(text):
#     text = re.sub(r"[Â°Ï„Âµ~|â€”_]{2,}", "", text)
#     text = re.sub(r"\s+", " ", text)
#     return text.strip()


# DATA_DIR = Path("data")
# INDEX_PATH = DATA_DIR / "vectorstore"
# METADATA_DIR = DATA_DIR / "metadata"
# for p in [METADATA_DIR, INDEX_PATH]:
#     p.mkdir(parents=True, exist_ok=True)

# with st.spinner("â³ Ø¯Ø± Ø­Ø§Ù„ Ø¨ÛŒØ¯Ø§Ø± Ú©Ø±Ø¯Ù† Ù…Ø¯Ù„â€ŒÙ‡Ø§..."):
#     embeddings, llm_engine, prompt_template, (rerank_m, rerank_t) = (
#         setup_llm_and_embeddings()
#     )

# if "vectorstore" not in st.session_state:
#     st.session_state.vectorstore = load_vectorstore_on_gpu(
#         str(INDEX_PATH), embeddings, 1
#     )

# if "full_texts" not in st.session_state:
#     st.session_state.full_texts = {}
#     for meta_file in METADATA_DIR.glob("*.json"):
#         with open(meta_file, "r") as f:
#             data = json.load(f)
#             if "full_content" in data:
#                 st.session_state.full_texts[data["name"]] = data["full_content"]

# # --- Ù…Ø¯ÛŒØ±ÛŒØª Ø³Ø§ÛŒØ¯Ø¨Ø§Ø± ---
# with st.sidebar:
#     st.markdown(
#         '<h3 style="color: white; direction: rtl;"><i class="bi bi-cpu-fill"></i> Ù¾Ù†Ù„ Ù…Ø¯ÛŒØ±ÛŒØª</h3>',
#         unsafe_allow_html=True,
#     )

#     with st.expander("ğŸ“Š Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø±", expanded=False):
#         for i in range(torch.cuda.device_count()):
#             used = torch.cuda.memory_reserved(i) / 1024**3
#             st.markdown(
#                 f'<div class="monitor-card"><i class="bi bi-gpu-card"></i> <b>Ú©Ø§Ø±Øª Ú¯Ø±Ø§ÙÛŒÚ© {i}</b><br>Ù…ØµØ±Ù: {used:.1f} GB</div>',
#                 unsafe_allow_html=True,
#             )
#         st.markdown(
#             f'<div class="monitor-card" style="background:#1e293b;"><i class="bi bi-cpu"></i> <b>Ø±Ù… Ø³ÛŒØ³ØªÙ…: {psutil.virtual_memory().percent}%</b></div>',
#             unsafe_allow_html=True,
#         )

#     st.divider()
#     files = st.file_uploader("Ø¢Ù¾Ù„ÙˆØ¯ PDF", type="pdf", accept_multiple_files=True)

#     if files and st.button("ğŸª„ Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´"):
#         new_docs = []
#         with st.status("ğŸš€ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡ÙˆØ´Ù…Ù†Ø¯...", expanded=True) as status:
#             for f in files:
#                 f_bytes = f.read()
#                 f_hash = hashlib.md5(f_bytes).hexdigest()
#                 meta_path = METADATA_DIR / f"{f_hash}.json"

#                 if meta_path.exists():
#                     st.toast(f"âœ… ÙØ§ÛŒÙ„ {f.name} Ù‚Ø¨Ù„Ø§Ù‹ OCR Ø´Ø¯Ù‡ Ø¨ÙˆØ¯.", icon="ğŸ’¾")
#                     with open(meta_path, "r") as m:
#                         combined_text = json.load(m)["full_content"]
#                 else:
#                     status.write(f"ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ†: {f.name}")
#                     imgs = convert_from_bytes(f_bytes, dpi=150)
#                     with ThreadPoolExecutor(max_workers=4) as exe:
#                         texts = list(
#                             exe.map(
#                                 lambda img: pytesseract.image_to_string(
#                                     img, lang="fas+eng"
#                                 ),
#                                 imgs,
#                             )
#                         )
#                     combined_text = clean_ocr_text("\n\n".join(texts))
#                     with open(meta_path, "w") as m:
#                         json.dump({"name": f.name, "full_content": combined_text}, m)
#                     st.toast(f"âœ¨ ÙØ§ÛŒÙ„ {f.name} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯.", icon="âœ…")

#                 st.session_state.full_texts[f.name] = combined_text
#                 new_docs.append(
#                     Document(page_content=combined_text, metadata={"source": f.name})
#                 )

#             if new_docs:
#                 splits = RecursiveCharacterTextSplitter(
#                     chunk_size=800, chunk_overlap=200
#                 ).split_documents(new_docs)
#                 vs = FAISS.from_documents(splits, embeddings)
#                 vs.index = faiss.index_gpu_to_cpu(vs.index)
#                 vs.save_local(str(INDEX_PATH))
#                 st.session_state.vectorstore = load_vectorstore_on_gpu(
#                     str(INDEX_PATH), embeddings, 1
#                 )
#                 status.update(label="âœ… Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø´Ø¯!", state="complete")
#                 st.rerun()

# # --- Ø¨Ø®Ø´ Ø§ØµÙ„ÛŒ Ú¯ÙØªÚ¯Ùˆ ---
# st.title("ğŸ¢ Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù…Ø¯ÛŒØ±ÛŒØª Ø¯Ø§Ù†Ø´")
# if "messages" not in st.session_state:
#     st.session_state.messages = []

# for m in st.session_state.messages:
#     with st.chat_message(m["role"], avatar="ğŸ‘¤" if m["role"] == "user" else "ğŸ¤–"):
#         st.markdown(
#             f'<div style="text-align: right; direction: rtl;">{m["content"]}</div>',
#             unsafe_allow_html=True,
#         )

# if prompt := st.chat_input("Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù¾Ø±Ø³ÛŒØ¯..."):
#     st.session_state.messages.append({"role": "user", "content": prompt})
#     with st.chat_message("user", avatar="ğŸ‘¤"):
#         st.markdown(
#             f'<div style="text-align: right; direction: rtl;">{prompt}</div>',
#             unsafe_allow_html=True,
#         )

#     with st.chat_message("assistant", avatar="ğŸ¤–"):
#         if st.session_state.vectorstore:
#             with st.spinner("ğŸ” Ø¯Ø± Ø­Ø§Ù„ ØªØ­Ù„ÛŒÙ„..."):
#                 docs = st.session_state.vectorstore.similarity_search(prompt, k=15)
#                 pairs = [[prompt, d.page_content] for d in docs]
#                 inputs = rerank_t(
#                     pairs,
#                     padding=True,
#                     truncation=True,
#                     return_tensors="pt",
#                     max_length=512,
#                 ).to("cuda:1")

#                 with torch.no_grad():
#                     scores = rerank_m(**inputs).logits.view(-1).float()

#                 context = "\n\n".join(
#                     [
#                         docs[i].page_content
#                         for i in torch.argsort(scores, descending=True)[:8]
#                     ]
#                 )
#                 chain = prompt_template | llm_engine
#                 response = chain.invoke({"context": context, "question": prompt})

#                 ans = (
#                     response.split("Ù¾Ø§Ø³Ø® Ø¬Ø§Ù…Ø¹ Ùˆ ØªØ´Ø±ÛŒØ­ÛŒ:")[-1].strip()
#                     if "Ù¾Ø§Ø³Ø® Ø¬Ø§Ù…Ø¹ Ùˆ ØªØ´Ø±ÛŒØ­ÛŒ:" in response
#                     else response
#                 )
#                 ans = re.sub(r"\*+", "", ans).strip()

#                 st.markdown(
#                     f'<div style="text-align: right; direction: rtl;">{ans}</div>',
#                     unsafe_allow_html=True,
#                 )
#                 st.session_state.messages.append({"role": "assistant", "content": ans})

#     torch.cuda.empty_cache()
#     gc.collect()


# import streamlit as st
# import torch, gc, json, hashlib, time, uuid, faiss, psutil, os, base64, re
# from pathlib import Path
# from pdf2image import convert_from_bytes
# import pytesseract
# from concurrent.futures import ThreadPoolExecutor
# from vector_manager import load_vectorstore_on_gpu
# from setup_models import setup_llm_and_embeddings
# from langchain_community.vectorstores import FAISS
# from langchain_text_splitters import RecursiveCharacterTextSplitter
# from langchain_core.documents import Document

# # --- Û±. ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ø§Ø³ØªØ§ÛŒÙ„ (ÙØ§ÛŒÙ„ style.py Ø´Ù…Ø§ Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ù…ÛŒâ€ŒÙ…Ø§Ù†Ø¯) ---
# from style import apply_custom_styles

# # --- Û². ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡ ØµÙØ­Ù‡ (Ø³Ø§ÛŒØ¯Ø¨Ø§Ø± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¨Ø§Ø² Ø§Ø³Øª) ---
# st.set_page_config(
#     page_title="Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø³Ø§Ø²Ù…Ø§Ù†ÛŒ", layout="wide", initial_sidebar_state="expanded"
# )

# # --- Û³. Ø§Ø¹Ù…Ø§Ù„ Ø§Ø³ØªØ§ÛŒÙ„â€ŒÙ‡Ø§ ---
# apply_custom_styles()


# def clean_ocr_text(text):
#     text = re.sub(r"[Â°Ï„Âµ~|â€”_]{2,}", "", text)
#     text = re.sub(r"\s+", " ", text)
#     return text.strip()


# DATA_DIR = Path("data")
# INDEX_PATH = DATA_DIR / "vectorstore"
# METADATA_DIR = DATA_DIR / "metadata"
# for p in [METADATA_DIR, INDEX_PATH]:
#     p.mkdir(parents=True, exist_ok=True)

# # Ù„ÙˆØ¯ Ú©Ø±Ø¯Ù† Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø­Ø§ÙØ¸Ù‡
# if "models" not in st.session_state:
#     with st.spinner("â³ Ø¯Ø± Ø­Ø§Ù„ Ø¨ÛŒØ¯Ø§Ø± Ú©Ø±Ø¯Ù† Ù…Ø¯Ù„â€ŒÙ‡Ø§..."):
#         st.session_state.models = setup_llm_and_embeddings()

# embeddings, llm_engine, prompt_template, (rerank_m, rerank_t) = st.session_state.models

# if "vectorstore" not in st.session_state:
#     st.session_state.vectorstore = load_vectorstore_on_gpu(
#         str(INDEX_PATH), embeddings, 1
#     )

# # --- Ù…Ø¯ÛŒØ±ÛŒØª Ø³Ø§ÛŒØ¯Ø¨Ø§Ø± ---
# with st.sidebar:
#     st.markdown(
#         '<h3 style="color: white; direction: rtl;"><i class="bi bi-cpu-fill"></i> Ù¾Ù†Ù„ Ù…Ø¯ÛŒØ±ÛŒØª</h3>',
#         unsafe_allow_html=True,
#     )

#     with st.expander("ğŸ“Š Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø±", expanded=False):
#         for i in range(torch.cuda.device_count()):
#             used = torch.cuda.memory_reserved(i) / 1024**3
#             st.markdown(
#                 f'<div class="monitor-card"><b>GPU {i}</b>: {used:.1f} GB</div>',
#                 unsafe_allow_html=True,
#             )
#         st.markdown(
#             f'<div class="monitor-card" style="background:#1e293b;"><b>RAM</b>: {psutil.virtual_memory().percent}%</div>',
#             unsafe_allow_html=True,
#         )

#     st.divider()
#     files = st.file_uploader("Ø¢Ù¾Ù„ÙˆØ¯ PDF", type="pdf", accept_multiple_files=True)

#     if files and st.button("ğŸª„ Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´"):
#         new_docs = []
#         with st.status("ğŸš€ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡ÙˆØ´Ù…Ù†Ø¯...", expanded=True) as status:
#             for f in files:
#                 f_bytes = f.read()
#                 f_hash = hashlib.md5(f_bytes).hexdigest()
#                 meta_path = METADATA_DIR / f"{f_hash}.json"

#                 if meta_path.exists():
#                     with open(meta_path, "r") as m:
#                         combined_text = json.load(m)["full_content"]
#                 else:
#                     imgs = convert_from_bytes(f_bytes, dpi=150)
#                     with ThreadPoolExecutor(max_workers=4) as exe:
#                         texts = list(
#                             exe.map(
#                                 lambda img: pytesseract.image_to_string(
#                                     img, lang="fas+eng"
#                                 ),
#                                 imgs,
#                             )
#                         )
#                     combined_text = clean_ocr_text("\n\n".join(texts))
#                     with open(meta_path, "w") as m:
#                         json.dump({"name": f.name, "full_content": combined_text}, m)

#                 new_docs.append(
#                     Document(page_content=combined_text, metadata={"source": f.name})
#                 )

#             if new_docs:
#                 splits = RecursiveCharacterTextSplitter(
#                     chunk_size=800, chunk_overlap=200
#                 ).split_documents(new_docs)
#                 vs = FAISS.from_documents(splits, embeddings)
#                 vs.save_local(str(INDEX_PATH))
#                 st.session_state.vectorstore = load_vectorstore_on_gpu(
#                     str(INDEX_PATH), embeddings, 1
#                 )
#                 st.rerun()

# # --- Ø¨Ø®Ø´ Ø§ØµÙ„ÛŒ Ú¯ÙØªÚ¯Ùˆ ---
# st.title("ğŸ¢ Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù…Ø¯ÛŒØ±ÛŒØª Ø¯Ø§Ù†Ø´")

# if "messages" not in st.session_state:
#     st.session_state.messages = []

# # Ù†Ù…Ø§ÛŒØ´ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ú†Øª (ÙÙ‚Ø· Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ú©Ø§Ø±Ø¨Ø± Ùˆ Ù…Ø¯Ù„)
# for m in st.session_state.messages:
#     with st.chat_message(m["role"], avatar="ğŸ‘¤" if m["role"] == "user" else "ğŸ¤–"):
#         st.markdown(
#             f'<div style="text-align: right; direction: rtl;">{m["content"]}</div>',
#             unsafe_allow_html=True,
#         )

# if prompt := st.chat_input("Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù¾Ø±Ø³ÛŒØ¯..."):
#     st.session_state.messages.append({"role": "user", "content": prompt})
#     with st.chat_message("user", avatar="ğŸ‘¤"):
#         st.markdown(
#             f'<div style="text-align: right; direction: rtl;">{prompt}</div>',
#             unsafe_allow_html=True,
#         )

#     with st.chat_message("assistant", avatar="ğŸ¤–"):
#         if st.session_state.vectorstore:
#             with st.spinner("ğŸ” Ø¯Ø± Ø­Ø§Ù„ ØªØ­Ù„ÛŒÙ„ Ø¹Ù…ÛŒÙ‚ Ù…Ø³ØªÙ†Ø¯Ø§Øª..."):
#                 # Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Û±Ûµ Ù…ÙˆØ±Ø¯ Ø¨Ø±Ø§ÛŒ Ø¯Ù‚Øª Ø¯Ø± Ø±Ù†Ú©ÛŒÙ†Ú¯
#                 docs = st.session_state.vectorstore.similarity_search(prompt, k=15)

#                 # Ù…Ø±Ø­Ù„Ù‡ Rerank Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¨Ù‡ØªØ±ÛŒÙ† Ù‚Ø·Ø¹Ø§Øª
#                 pairs = [[prompt, d.page_content] for d in docs]
#                 inputs = rerank_t(
#                     pairs,
#                     padding=True,
#                     truncation=True,
#                     return_tensors="pt",
#                     max_length=512,
#                 ).to("cuda:1")
#                 with torch.no_grad():
#                     scores = rerank_m(**inputs).logits.view(-1).float()

#                 # Ø§Ù†ØªØ®Ø§Ø¨ Û¸ Ù‚Ø·Ø¹Ù‡ Ø¨Ø±ØªØ± (Ù…Ø§Ù†Ù†Ø¯ Ú©Ø¯ Ø§ÙˆÙ„ Ø´Ù…Ø§ Ø¨Ø±Ø§ÛŒ Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§)
#                 best_indices = torch.argsort(scores, descending=True)[:8]
#                 context = "\n\n".join([docs[i].page_content for i in best_indices])

#                 # Ø§Ø¬Ø±Ø§ÛŒ Ø²Ù†Ø¬ÛŒØ±Ù‡ Ù¾Ø§Ø³Ø®Ú¯ÙˆÛŒÛŒ
#                 chain = prompt_template | llm_engine
#                 response = chain.invoke({"context": context, "question": prompt})

#                 # ØªÙ…ÛŒØ²Ú©Ø§Ø±ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù¾Ø§Ø³Ø® (Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø­Ø°Ù Ù…Ø­ØªÙˆØ§ Ùˆ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø³ØªÙˆØ±Ø§Øª Ø³ÛŒØ³ØªÙ…)
#                 ans = response
#                 # Ø§Ú¯Ø± Ù…Ø¯Ù„ ØªÚ¯â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ ØªÙˆÙ„ÛŒØ¯ Ú©Ø±Ø¯ØŒ Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ ÙÛŒÙ„ØªØ± Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
#                 if "Ù¾Ø§Ø³Ø® Ø¬Ø§Ù…Ø¹ Ùˆ ØªØ´Ø±ÛŒØ­ÛŒ:" in ans:
#                     ans = ans.split("Ù¾Ø§Ø³Ø® Ø¬Ø§Ù…Ø¹ Ùˆ ØªØ´Ø±ÛŒØ­ÛŒ:")[-1]
#                 elif "Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ:" in ans:
#                     ans = ans.split("Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ:")[-1]

#                 # Ø­Ø°Ù Ù†ÙˆÛŒØ²Ù‡Ø§ÛŒ Ø§Ø­ØªÙ…Ø§Ù„ÛŒ Ùˆ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù†Ù‡Ø§ÛŒÛŒ
#                 ans = re.sub(
#                     r"System:.*?\n", "", ans, flags=re.DOTALL
#                 )  # Ø­Ø°Ù Ø¯Ø³ØªÙˆØ±Ø§Øª Ø³ÛŒØ³ØªÙ… Ø§Ø² Ø®Ø±ÙˆØ¬ÛŒ
#                 ans = re.sub(r"Human:.*?\n", "", ans, flags=re.DOTALL)  # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø± Ø³ÙˆØ§Ù„
#                 ans = re.sub(r"\*+", "", ans).strip()

#                 st.markdown(
#                     f'<div style="text-align: right; direction: rtl;">{ans}</div>',
#                     unsafe_allow_html=True,
#                 )
#                 st.session_state.messages.append({"role": "assistant", "content": ans})

#     # Ø¢Ø²Ø§Ø¯ Ú©Ø±Ø¯Ù† Ø­Ø§ÙØ¸Ù‡ Ú¯Ø±Ø§ÙÛŒÚ©ÛŒ Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª Ø¹Ù…Ù„ Ù…Ø³ØªÙ…Ø±
#     torch.cuda.empty_cache()
#     gc.collect()


# import streamlit as st
# import torch, gc, json, hashlib, time, faiss, psutil, os, re
# from pathlib import Path
# from pdf2image import convert_from_bytes
# import pytesseract
# from concurrent.futures import ThreadPoolExecutor
# from vector_manager import load_vectorstore_on_gpu
# from setup_models import setup_llm_and_embeddings
# from langchain_community.vectorstores import FAISS
# from langchain_text_splitters import RecursiveCharacterTextSplitter
# from langchain_core.documents import Document

# # --- Û±. ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø³ÛŒØ³ØªÙ…ÛŒ Ùˆ Ø¸Ø§Ù‡Ø±ÛŒ ---
# from style import apply_custom_styles

# st.set_page_config(
#     page_title="Enterprise AI Knowledge Hub",
#     layout="wide",
#     initial_sidebar_state="expanded",
# )
# apply_custom_styles()

# # --- Û². Ø²ÛŒØ±Ø³Ø§Ø®Øª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø´ØªØ±Ú© (Global Storage) ---
# DATA_DIR = Path("data")
# INDEX_PATH = DATA_DIR / "vectorstore"
# METADATA_DIR = DATA_DIR / "metadata"
# for p in [METADATA_DIR, INDEX_PATH]:
#     p.mkdir(parents=True, exist_ok=True)


# @st.cache_resource
# def initialize_engine():
#     """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±ÙˆÛŒ GPU Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§Ù… Ú©Ø§Ø±Ø¨Ø±Ø§Ù†"""
#     return setup_llm_and_embeddings()


# models = initialize_engine()
# embeddings, llm_engine, prompt_template, (rerank_m, rerank_t) = models

# # Ù„ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø§ÛŒÙ†Ø¯Ú©Ø³ Ù…Ø±Ú©Ø²ÛŒ
# if "vectorstore" not in st.session_state:
#     if (INDEX_PATH / "index.faiss").exists():
#         st.session_state.vectorstore = load_vectorstore_on_gpu(
#             str(INDEX_PATH), embeddings, 1
#         )
#     else:
#         st.session_state.vectorstore = None


# # --- Û³. ØªØ§Ø¨Ø¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ùˆ Ú†Ú© Ú©Ø±Ø¯Ù† ØªÚ©Ø±Ø§Ø± Ø¯Ø± Ú©Ù„ Ø´Ø¨Ú©Ù‡ ---
# def process_file_globally(file_obj):
#     """Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ÙØ§ÛŒÙ„ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù…Ø±Ú©Ø²ÛŒ Ù‚Ø¨Ù„ Ø§Ø² OCR"""
#     f_bytes = file_obj.read()
#     f_hash = hashlib.md5(f_bytes).hexdigest()
#     meta_path = METADATA_DIR / f"{f_hash}.json"

#     # Ø§Ú¯Ø± ÙØ§ÛŒÙ„ Ù‚Ø¨Ù„Ø§Ù‹ ØªÙˆØ³Ø· Ù‡Ø± Ú©Ø³ÛŒ Ø¢Ù¾Ù„ÙˆØ¯ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯
#     if meta_path.exists():
#         with open(meta_path, "r", encoding="utf-8") as m:
#             data = json.load(m)
#             return data["full_content"], True  # ÙØ§ÛŒÙ„ Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³Øª

#     # Ø§Ú¯Ø± ÙØ§ÛŒÙ„ Ø¬Ø¯ÛŒØ¯ Ø§Ø³ØªØŒ Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ OCR
#     imgs = convert_from_bytes(f_bytes, dpi=120)
#     with ThreadPoolExecutor() as executor:
#         texts = list(
#             executor.map(
#                 lambda img: pytesseract.image_to_string(img, lang="fas+eng"), imgs
#             )
#         )

#     combined_text = "\n\n".join(texts)
#     # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù…Ø±Ú©Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¨Ù‚ÛŒÙ‡ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†
#     with open(meta_path, "w", encoding="utf-8") as m:
#         json.dump(
#             {"name": file_obj.name, "full_content": combined_text, "hash": f_hash}, m
#         )

#     return combined_text, False


# # --- Û´. Ù…Ø¯ÛŒØ±ÛŒØª Ø¢Ù¾Ù„ÙˆØ¯ Ùˆ Ø§Ø¹Ù„Ø§Ù†â€ŒÙ‡Ø§ÛŒ ØªÙˆØ³Øª (Toast) ---
# with st.sidebar:
#     st.markdown("### ğŸ’  Ù…Ø¯ÛŒØ±ÛŒØª Ù…ØªÙ…Ø±Ú©Ø² Ø§Ø³Ù†Ø§Ø¯")
#     uploaded_files = st.file_uploader(
#         "ÙØ§ÛŒÙ„ PDF Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯", type="pdf", accept_multiple_files=True
#     )

#     if uploaded_files and st.button("ğŸš€ Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ù‡Ù…Ú¯Ø§Ù…â€ŒØ³Ø§Ø²ÛŒ", use_container_width=True):
#         new_docs = []
#         with st.status("Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¶Ø¹ÛŒØª ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø¯Ø± Ø³Ø±ÙˆØ±...", expanded=True) as status:
#             for f in uploaded_files:
#                 content, is_already_exists = process_file_globally(f)

#                 if is_already_exists:
#                     # Ù†Ù…Ø§ÛŒØ´ Ù†ÙˆØªÛŒÙÛŒÚ©ÛŒØ´Ù† ØªÙˆØ³Øª Ú©Ù‡ ÙØ§ÛŒÙ„ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³Øª
#                     st.toast(
#                         f"ÙØ§ÛŒÙ„ '{f.name}' Ù‚Ø¨Ù„Ø§Ù‹ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ Ùˆ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ø§Ø³Øª.", icon="âœ…"
#                     )
#                 else:
#                     st.write(f"â³ ÙØ§ÛŒÙ„ Ø¬Ø¯ÛŒØ¯ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯. Ø¯Ø± Ø­Ø§Ù„ OCR: {f.name}")
#                     new_docs.append(
#                         Document(page_content=content, metadata={"source": f.name})
#                     )

#             if new_docs:
#                 status.update(label="Ø¯Ø± Ø­Ø§Ù„ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ù‡ÙˆØ´Ù…Ù†Ø¯...")
#                 splitter = RecursiveCharacterTextSplitter(
#                     chunk_size=750, chunk_overlap=120
#                 )
#                 splits = splitter.split_documents(new_docs)

#                 if st.session_state.vectorstore is None:
#                     st.session_state.vectorstore = FAISS.from_documents(
#                         splits, embeddings
#                     )
#                 else:
#                     st.session_state.vectorstore.add_documents(splits)

#                 st.session_state.vectorstore.save_local(str(INDEX_PATH))
#                 st.success("ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ø¯Ø§Ù†Ø´ Ø³ÛŒØ³ØªÙ… Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù†Ø¯.")

#             status.update(label="Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ú©Ø§Ù…Ù„Ø§Ù‹ Ù‡Ù…Ú¯Ø§Ù… Ø§Ø³Øª.", state="complete")

# # --- Ûµ. Ø¨Ø®Ø´ Ú†Øª Ùˆ Ø§Ø³ØªØ±ÛŒÙ… Ù¾Ø§Ø³Ø® ---
# st.title("ğŸ¢ Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø³Ø§Ø²Ù…Ø§Ù†ÛŒ")

# if "messages" not in st.session_state:
#     st.session_state.messages = []

# for m in st.session_state.messages:
#     with st.chat_message(m["role"]):
#         st.markdown(
#             f'<div dir="rtl" style="text-align:right">{m["content"]}</div>',
#             unsafe_allow_html=True,
#         )

# if prompt := st.chat_input("Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù¾Ø±Ø³ÛŒØ¯..."):
#     st.session_state.messages.append({"role": "user", "content": prompt})
#     with st.chat_message("user"):
#         st.markdown(
#             f'<div dir="rtl" style="text-align:right">{prompt}</div>',
#             unsafe_allow_html=True,
#         )

#     with st.chat_message("assistant"):
#         if st.session_state.vectorstore:
#             # Ú©Ø§Ù†ØªÛŒÙ†Ø± Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ±ÛŒÙ… Ú©Ù„Ù…Ù‡ Ø¨Ù‡ Ú©Ù„Ù…Ù‡
#             resp_container = st.empty()

#             # Ø¬Ø³ØªØ¬ÙˆÛŒ Ø³Ø±ÛŒØ¹ GPU
#             docs = st.session_state.vectorstore.similarity_search(prompt, k=15)

#             # Ø±Ù†Ú©ÛŒÙ†Ú¯ Ù…Ø¬Ø¯Ø¯ (Rerank) Ø±ÙˆÛŒ GPU
#             pairs = [[prompt, d.page_content] for d in docs]
#             inputs = rerank_t(
#                 pairs,
#                 padding=True,
#                 truncation=True,
#                 return_tensors="pt",
#                 max_length=512,
#             ).to("cuda:1")

#             with torch.no_grad():
#                 scores = rerank_m(**inputs).logits.view(-1).float()
#                 best_indices = torch.argsort(scores, descending=True)[:7]
#                 context = "\n\n".join([docs[i].page_content for i in best_indices])

#             # ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ù‡ ØµÙˆØ±Øª Streaming (Ø¨Ø±Ø§ÛŒ Ø­Ø°Ù ØªØ§Ø®ÛŒØ± Ø¸Ø§Ù‡Ø±ÛŒ)
#             full_ans = ""
#             for chunk in llm_engine.stream(
#                 prompt_template.format(context=context, question=prompt)
#             ):
#                 # Ø­Ø°Ù Ú©Ù„Ù…Ø§Øª Ø³ÛŒØ³ØªÙ…ÛŒ Ù†Ø§Ø®ÙˆØ§Ø³ØªÙ‡
#                 chunk = re.sub(
#                     r"(System:|Human:|Assistant:).*", "", chunk, flags=re.DOTALL
#                 )
#                 full_ans += chunk
#                 resp_container.markdown(
#                     f'<div dir="rtl" style="text-align:right">{full_ans} â–Œ</div>',
#                     unsafe_allow_html=True,
#                 )

#             resp_container.markdown(
#                 f'<div dir="rtl" style="text-align:right">{full_ans}</div>',
#                 unsafe_allow_html=True,
#             )
#             st.session_state.messages.append({"role": "assistant", "content": full_ans})

#             # Ø¢Ø²Ø§Ø¯Ø³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡
#             torch.cuda.empty_cache()
#             gc.collect()
#         else:
#             st.warning("Ø§Ø¨ØªØ¯Ø§ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø±Ø§ Ø¢Ù¾Ù„ÙˆØ¯ ÛŒØ§ Ù‡Ù…Ú¯Ø§Ù…â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯.")


# import os

# os.environ["TRANSFORMERS_OFFLINE"] = "1"
# os.environ["HF_DATASETS_OFFLINE"] = "1"

# import streamlit as st
# import torch, gc, json, hashlib, time, faiss, psutil, os, re
# from pathlib import Path
# from pdf2image import convert_from_bytes
# import pytesseract
# from concurrent.futures import ThreadPoolExecutor
# from vector_manager import load_vectorstore_on_gpu
# from setup_models import setup_llm_and_embeddings
# from langchain_community.vectorstores import FAISS
# from langchain_text_splitters import RecursiveCharacterTextSplitter
# from langchain_core.documents import Document

# # --- Û±. ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ùˆ Ø§Ø³ØªØ§ÛŒÙ„ ---
# from style import apply_custom_styles

# st.set_page_config(
#     page_title="Enterprise AI Knowledge Hub",
#     layout="wide",
#     initial_sidebar_state="expanded",
# )
# apply_custom_styles()

# # --- Û². Ù…Ø³ÛŒØ±Ù‡Ø§ Ùˆ Ù…Ø¯Ù„â€ŒÙ‡Ø§ ---
# DATA_DIR = Path("data")
# INDEX_PATH = DATA_DIR / "vectorstore"
# METADATA_DIR = DATA_DIR / "metadata"
# for p in [METADATA_DIR, INDEX_PATH]:
#     p.mkdir(parents=True, exist_ok=True)


# @st.cache_resource
# def initialize_engine():
#     return setup_llm_and_embeddings()


# models = initialize_engine()
# embeddings, llm_engine, prompt_template, (rerank_m, rerank_t) = models

# if "vectorstore" not in st.session_state:
#     if (INDEX_PATH / "index.faiss").exists():
#         st.session_state.vectorstore = load_vectorstore_on_gpu(
#             str(INDEX_PATH), embeddings, 1
#         )
#     else:
#         st.session_state.vectorstore = None


# # --- Û³. Ù¾Ø±Ø¯Ø§Ø²Ø´ OCR Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø§ Ú©Ø´ Ø¬Ù‡Ø§Ù†ÛŒ ---
# def process_file_globally(file_obj):
#     file_obj.seek(0)
#     f_bytes = file_obj.read()
#     f_hash = hashlib.md5(f_bytes).hexdigest()
#     meta_path = METADATA_DIR / f"{f_hash}.json"

#     if meta_path.exists():
#         with open(meta_path, "r", encoding="utf-8") as m:
#             data = json.load(m)
#             return data["full_content"], True

#     imgs = convert_from_bytes(f_bytes, dpi=120)
#     with ThreadPoolExecutor() as executor:
#         texts = list(
#             executor.map(
#                 lambda img: pytesseract.image_to_string(img, lang="fas+eng"), imgs
#             )
#         )

#     combined_text = "\n\n".join(texts)
#     with open(meta_path, "w", encoding="utf-8") as m:
#         json.dump(
#             {"name": file_obj.name, "full_content": combined_text, "hash": f_hash}, m
#         )

#     return combined_text, False


# # --- Û´. Ø³Ø§ÛŒØ¯Ø¨Ø§Ø± Ù…Ø¯ÛŒØ±ÛŒØªÛŒ Ùˆ Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ ---
# with st.sidebar:
#     st.markdown(
#         '<h3 style="color: white; direction: rtl;"><i class="bi bi-cpu-fill"></i> Ù¾Ù†Ù„ Ù…Ø¯ÛŒØ±ÛŒØª</h3>',
#         unsafe_allow_html=True,
#     )

#     with st.expander("ğŸ“Š Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø± (GPU/RAM)", expanded=True):
#         for i in range(torch.cuda.device_count()):
#             used = torch.cuda.memory_reserved(i) / 1024**3
#             st.markdown(
#                 f'<div class="monitor-card"><b>GPU {i}:</b> {used:.1f} GB</div>',
#                 unsafe_allow_html=True,
#             )
#         st.markdown(
#             f'<div class="monitor-card" style="background:#1e293b;"><b>RAM:</b> {psutil.virtual_memory().percent}%</div>',
#             unsafe_allow_html=True,
#         )

#     st.divider()
#     uploaded_files = st.file_uploader(
#         "ÙØ§ÛŒÙ„ PDF (ÛŒÚ© ÛŒØ§ Ú†Ù†Ø¯Ú¯Ø§Ù†Ù‡)", type="pdf", accept_multiple_files=True
#     )

#     if uploaded_files and st.button("ğŸš€ Ù‡Ù…Ú¯Ø§Ù…â€ŒØ³Ø§Ø²ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³", use_container_width=True):
#         new_docs = []
#         with st.status("Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø±Ø±Ø³ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§...", expanded=True) as status:
#             for f in uploaded_files:
#                 content, is_already_exists = process_file_globally(f)
#                 if is_already_exists:
#                     st.toast(f"ÙØ§ÛŒÙ„ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡ Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³Øª: {f.name}", icon="âœ…")
#                 else:
#                     new_docs.append(
#                         Document(page_content=content, metadata={"source": f.name})
#                     )

#             if new_docs:
#                 splitter = RecursiveCharacterTextSplitter(
#                     chunk_size=1000, chunk_overlap=200
#                 )
#                 splits = splitter.split_documents(new_docs)
#                 if st.session_state.vectorstore is None:
#                     st.session_state.vectorstore = FAISS.from_documents(
#                         splits, embeddings
#                     )
#                 else:
#                     st.session_state.vectorstore.add_documents(splits)
#                 st.session_state.vectorstore.save_local(str(INDEX_PATH))
#             status.update(label="Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡â€ŒØ±ÙˆØ² Ø´Ø¯!", state="complete")

# # --- Ûµ. Ú†Øªâ€ŒØ¨Ø§Øª Ø¨Ø§ Ù…Ù†Ø·Ù‚ Ù¾Ø§Ø³Ø®Ú¯ÙˆÛŒÛŒ Ø¹Ù…ÛŒÙ‚ (Deep RAG) ---
# st.title("ğŸ¢ Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù…Ø¯ÛŒØ±ÛŒØª Ø¯Ø§Ù†Ø´")

# if "messages" not in st.session_state:
#     st.session_state.messages = []

# for m in st.session_state.messages:
#     with st.chat_message(m["role"]):
#         st.markdown(
#             f'<div dir="rtl" style="text-align:right">{m["content"]}</div>',
#             unsafe_allow_html=True,
#         )

# if prompt := st.chat_input("Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø§ Ø¬Ø²Ø¦ÛŒØ§Øª Ø¨Ù¾Ø±Ø³ÛŒØ¯..."):
#     st.session_state.messages.append({"role": "user", "content": prompt})
#     with st.chat_message("user"):
#         st.markdown(
#             f'<div dir="rtl" style="text-align:right">{prompt}</div>',
#             unsafe_allow_html=True,
#         )

#     with st.chat_message("assistant"):
#         if st.session_state.vectorstore:
#             resp_placeholder = st.empty()

#             # Û±. Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ùˆ Ø±Ù†Ú©ÛŒÙ†Ú¯ (Ø¨Ø³ÛŒØ§Ø± Ø¯Ù‚ÛŒÙ‚)
#             docs = st.session_state.vectorstore.similarity_search(prompt, k=15)
#             pairs = [[prompt, d.page_content] for d in docs]
#             inputs = rerank_t(
#                 pairs,
#                 padding=True,
#                 truncation=True,
#                 return_tensors="pt",
#                 max_length=512,
#             ).to("cuda:1")

#             with torch.no_grad():
#                 scores = rerank_m(**inputs).logits.view(-1).float()
#                 # Ø§Ù†ØªØ®Ø§Ø¨ Û±Û° Ù‚Ø·Ø¹Ù‡ Ø¨Ø±ØªØ± Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø´ØªÙ† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø­Ø¯Ø§Ú©Ø«Ø±ÛŒ
#                 best_indices = torch.argsort(scores, descending=True)[:10]
#                 selected_docs = [docs[i] for i in best_indices]
#                 context_text = "\n\n".join([d.page_content for d in selected_docs])
#                 sources = list(
#                     set([d.metadata.get("source", "Ù†Ø§Ø´Ù†Ø§Ø³") for d in selected_docs])
#                 )

#             # Û². Ø³Ø§Ø®Øª Ù¾Ø±ÙˆÙ…Ù¾Øª Ù…Ù‡Ù†Ø¯Ø³ÛŒ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø® Ø·ÙˆÙ„Ø§Ù†ÛŒ
#             # Ù†Ú©ØªÙ‡: Ø§Ú¯Ø± Ú©Ø§Ø±Ø¨Ø± "Ù…ØªÙ† Ú©Ø§Ù…Ù„" Ø®ÙˆØ§Ø³ØªØŒ Ù…Ø³ØªÙ‚ÛŒÙ… Ø§Ø² Ù…ØªØ§Ø¯ÛŒØªØ§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†ÛŒÙ…
#             if any(word in prompt for word in ["Ù…ØªÙ† Ú©Ø§Ù…Ù„", "Ú©Ù„ ÙØ§ÛŒÙ„", "ØªÙ…ÙˆÙ… Ù…ØªÙ†"]):
#                 full_raw = ""
#                 for meta_file in METADATA_DIR.glob("*.json"):
#                     with open(meta_file, "r", encoding="utf-8") as m:
#                         data = json.load(m)
#                         full_raw += f"\n--- Ù…Ø­ØªÙˆØ§ÛŒ ÙØ§ÛŒÙ„: {data['name']} ---\n{data['full_content']}\n"
#                 full_ans = full_raw if full_raw else "Ù…ØªÙ†ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯."
#             else:
#                 # Ø¯Ø³ØªÙˆØ± ØµØ±ÛŒØ­ Ø¨Ù‡ Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ù‡ÛŒØ² Ø§Ø² Ù¾Ø§Ø³Ø® Ú©ÙˆØªØ§Ù‡
#                 enhanced_prompt = f"""Ø´Ù…Ø§ ÛŒÚ© Ú©Ø§Ø±Ø´Ù†Ø§Ø³ Ø®Ø¨Ø±Ù‡ ØªØ­Ù„ÛŒÙ„ Ù…Ø­ØªÙˆØ§ Ù‡Ø³ØªÛŒØ¯.
#                 Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø²ÛŒØ±ØŒ ÛŒÚ© Ù¾Ø§Ø³Ø® **Ø¨Ø³ÛŒØ§Ø± Ø¬Ø§Ù…Ø¹ØŒ Ù…ÙØµÙ„ Ùˆ Ø¨Ø§ ØªÙ…Ø§Ù… Ø¬Ø²Ø¦ÛŒØ§Øª** Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯.
#                 ØªØ£Ú©ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ù…: Ù¾Ø§Ø³Ø® Ù†Ø¨Ø§ÛŒØ¯ Ú©ÙˆØªØ§Ù‡ Ø¨Ø§Ø´Ø¯. ØªÙ…Ø§Ù… Ù†Ú©Ø§Øª Ù…Ù‡Ù… Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù…ØªÙ† Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ùˆ ØªØ­Ù„ÛŒÙ„ Ú©Ù†ÛŒØ¯.
#                 Ø§Ú¯Ø± Ø³ÙˆØ§Ù„ÛŒ Ù¾Ø±Ø³ÛŒØ¯Ù‡ Ø´Ø¯Ù‡ Ú©Ù‡ Ø¯Ø± Ù…ØªÙ† Ù†ÛŒØ³ØªØŒ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯: 'Ø¯Ø± Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¨Ù‡ Ø§ÛŒÙ† Ù…ÙˆØ±Ø¯ Ø§Ø´Ø§Ø±Ù‡ Ù†Ø´Ø¯Ù‡'.

#                 Ù…Ø­ØªÙˆØ§ÛŒ Ø§Ø³Ù†Ø§Ø¯:
#                 {context_text}

#                 Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±: {prompt}

#                 Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ùˆ Ú©Ø§Ù…Ù„:"""

#                 full_ans = ""
#                 for chunk in llm_engine.stream(enhanced_prompt):
#                     # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù†ÙˆÛŒØ²Ù‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ
#                     chunk = re.sub(
#                         r"(System:|Assistant:|Human:|User:).*?",
#                         "",
#                         chunk,
#                         flags=re.IGNORECASE,
#                     )
#                     full_ans += chunk
#                     resp_placeholder.markdown(
#                         f'<p dir="rtl" style="text-align:right">{full_ans} â–Œ</p>',
#                         unsafe_allow_html=True,
#                     )

#             # Û³. Ù†Ù…Ø§ÛŒØ´ Ù†Ù‡Ø§ÛŒÛŒ Ù‡Ù…Ø±Ø§Ù‡ Ø¨Ø§ ØªÚ¯ Ù…Ù†Ø§Ø¨Ø¹
#             source_html = " ".join(
#                 [
#                     f'<span style="background:#1e293b; padding:2px 8px; border-radius:5px; font-size:12px; margin-right:5px;">ğŸ“„ {s}</span>'
#                     for s in sources
#                 ]
#             )
#             final_output = f'<div dir="rtl" style="text-align:right;">{full_ans}<br><br><hr>{source_html}</div>'
#             resp_placeholder.markdown(final_output, unsafe_allow_html=True)

#             st.session_state.messages.append({"role": "assistant", "content": full_ans})
#             torch.cuda.empty_cache()
#             gc.collect()
#         else:
#             st.warning(
#                 "Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø®Ø§Ù„ÛŒ Ø§Ø³Øª! Ù„Ø·ÙØ§Ù‹ Ø§Ø¨ØªØ¯Ø§ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø¯Ø± Ø³Ø§ÛŒØ¯Ø¨Ø§Ø± Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯."
#             )


import os

os.environ["TRANSFORMERS_OFFLINE"] = "1"
os.environ["HF_DATASETS_OFFLINE"] = "1"

import streamlit as st
import torch, gc, json, hashlib, time, faiss, psutil, os, re
from pathlib import Path
from pdf2image import convert_from_bytes
import pytesseract
from concurrent.futures import ThreadPoolExecutor
from vector_manager import load_vectorstore_on_gpu
from setup_models import setup_llm_and_embeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

# --- Û±. ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ùˆ Ø§Ø³ØªØ§ÛŒÙ„ ---
from style import apply_custom_styles

st.set_page_config(
    page_title="Enterprise AI Knowledge Hub",
    layout="wide",
    initial_sidebar_state="expanded",
)
apply_custom_styles()

# --- Û². Ù…Ø³ÛŒØ±Ù‡Ø§ Ùˆ Ù…Ø¯Ù„â€ŒÙ‡Ø§ ---
DATA_DIR = Path("data")
INDEX_PATH = DATA_DIR / "vectorstore"
METADATA_DIR = DATA_DIR / "metadata"
for p in [METADATA_DIR, INDEX_PATH]:
    p.mkdir(parents=True, exist_ok=True)


@st.cache_resource
def initialize_engine():
    return setup_llm_and_embeddings()


models = initialize_engine()
embeddings, llm_engine, prompt_template, (rerank_m, rerank_t) = models

if "vectorstore" not in st.session_state:
    if (INDEX_PATH / "index.faiss").exists():
        st.session_state.vectorstore = load_vectorstore_on_gpu(
            str(INDEX_PATH), embeddings, 1
        )
    else:
        st.session_state.vectorstore = None


# --- Û³. Ù¾Ø±Ø¯Ø§Ø²Ø´ OCR Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø§ Ú©Ø´ Ø¬Ù‡Ø§Ù†ÛŒ ---
def process_file_globally(file_obj):
    file_obj.seek(0)
    f_bytes = file_obj.read()
    f_hash = hashlib.md5(f_bytes).hexdigest()
    meta_path = METADATA_DIR / f"{f_hash}.json"

    if meta_path.exists():
        with open(meta_path, "r", encoding="utf-8") as m:
            data = json.load(m)
            return data["full_content"], True

    imgs = convert_from_bytes(f_bytes, dpi=120)
    with ThreadPoolExecutor() as executor:
        texts = list(
            executor.map(
                lambda img: pytesseract.image_to_string(img, lang="fas+eng"), imgs
            )
        )

    combined_text = "\n\n".join(texts)
    with open(meta_path, "w", encoding="utf-8") as m:
        json.dump(
            {"name": file_obj.name, "full_content": combined_text, "hash": f_hash}, m
        )

    return combined_text, False


# --- Û´. Ø³Ø§ÛŒØ¯Ø¨Ø§Ø± Ù…Ø¯ÛŒØ±ÛŒØªÛŒ Ùˆ Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ ---
with st.sidebar:
    st.markdown(
        '<h3 style="color: white; direction: rtl;"><i class="bi bi-cpu-fill"></i> Ù¾Ù†Ù„ Ù…Ø¯ÛŒØ±ÛŒØª</h3>',
        unsafe_allow_html=True,
    )

    with st.expander("ğŸ“Š Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø± (GPU/RAM)", expanded=True):
        for i in range(torch.cuda.device_count()):
            used = torch.cuda.memory_reserved(i) / 1024**3
            st.markdown(
                f'<div class="monitor-card"><b>GPU {i}:</b> {used:.1f} GB</div>',
                unsafe_allow_html=True,
            )
        st.markdown(
            f'<div class="monitor-card" style="background:#1e293b;"><b>RAM:</b> {psutil.virtual_memory().percent}%</div>',
            unsafe_allow_html=True,
        )

    st.divider()
    uploaded_files = st.file_uploader(
        "ÙØ§ÛŒÙ„ PDF (ÛŒÚ© ÛŒØ§ Ú†Ù†Ø¯Ú¯Ø§Ù†Ù‡)", type="pdf", accept_multiple_files=True
    )

    #     if uploaded_files and st.button("ğŸš€ Ù‡Ù…Ú¯Ø§Ù…â€ŒØ³Ø§Ø²ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³", use_container_width=True):
    #         new_docs = []
    #         with st.status("Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø±Ø±Ø³ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§...", expanded=True) as status:
    #             for f in uploaded_files:
    #                 content, is_already_exists = process_file_globally(f)
    #                 if is_already_exists:
    #                     st.toast(f"ÙØ§ÛŒÙ„ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡ Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³Øª: {f.name}", icon="âœ…")
    #                 else:
    #                     new_docs.append(
    #                         Document(page_content=content, metadata={"source": f.name})
    #                     )

    #             # if new_docs:
    #             #     splitter = RecursiveCharacterTextSplitter(
    #             #         chunk_size=1000, chunk_overlap=200
    #             #     )
    #             #     splits = splitter.split_documents(new_docs)
    #             #     if st.session_state.vectorstore is None:
    #             #         st.session_state.vectorstore = FAISS.from_documents(
    #             #             splits, embeddings
    #             #         )
    #             #     else:
    #             #         st.session_state.vectorstore.add_documents(splits)
    #             #     st.session_state.vectorstore.save_local(str(INDEX_PATH))

    #             # status.update(label="Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡â€ŒØ±ÙˆØ² Ø´Ø¯!", state="complete")

    #             if new_docs:
    #                 splitter = RecursiveCharacterTextSplitter(
    #                     chunk_size=1000, chunk_overlap=200
    #                 )
    #                 splits = splitter.split_documents(new_docs)

    #                 if st.session_state.vectorstore is None:
    #                     st.session_state.vectorstore = FAISS.from_documents(
    #                         splits, embeddings
    #                     )
    #                 else:
    #                     st.session_state.vectorstore.add_documents(splits)

    #                 # --- Ø¨Ø®Ø´ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² RuntimeError ---
    #                 # Û±. Ú¯Ø±ÙØªÙ† Ø§ÛŒÙ†Ø¯Ú©Ø³ ÙØ¹Ù„ÛŒ Ø§Ø² GPU
    #                 gpu_index = st.session_state.vectorstore.index

    #                 # Û². ØªØ¨Ø¯ÛŒÙ„ Ù…ÙˆÙ‚Øª Ø¨Ù‡ Ø§ÛŒÙ†Ø¯Ú©Ø³ CPU Ø¨Ø±Ø§ÛŒ Ù‚Ø§Ø¨Ù„ÛŒØª Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø±ÙˆÛŒ Ù‡Ø§Ø±Ø¯
    #                 st.session_state.vectorstore.index = faiss.index_gpu_to_cpu(gpu_index)

    #                 # Û³. Ø§Ù†Ø¬Ø§Ù… Ø¹Ù…Ù„ÛŒØ§Øª Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ
    #                 st.session_state.vectorstore.save_local(str(INDEX_PATH))

    #                 # Û´. Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø¨Ù‡ GPU Ø¨Ø±Ø§ÛŒ Ø­ÙØ¸ Ø³Ø±Ø¹Øª Ø¨Ø§Ù„Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ú†Øª
    #                 st.session_state.vectorstore.index = gpu_index
    #                 # --------------------------------------------------

    #             status.update(label="Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡â€ŒØ±ÙˆØ² Ø´Ø¯!", state="complete")

    # # --- Ûµ. Ú†Øªâ€ŒØ¨Ø§Øª Ø¨Ø§ Ù…Ù†Ø·Ù‚ Ù¾Ø§Ø³Ø®Ú¯ÙˆÛŒÛŒ Ø¹Ù…ÛŒÙ‚ (Deep RAG) ---
    # st.title("ğŸ¢ Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù…Ø¯ÛŒØ±ÛŒØª Ø¯Ø§Ù†Ø´")

    if uploaded_files and st.button("ğŸš€ Ù‡Ù…Ú¯Ø§Ù…â€ŒØ³Ø§Ø²ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³", use_container_width=True):
        new_docs = []
        with st.status("Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø±Ø±Ø³ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§...", expanded=True) as status:
            for f in uploaded_files:
                content, is_already_exists = process_file_globally(f)

                if is_already_exists:
                    st.toast(f"ÙØ§ÛŒÙ„ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡ Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³Øª: {f.name}", icon="âœ…")

                # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø­ØªÙˆØ§ Ø¨Ù‡ Ù„ÛŒØ³Øª (Ú†Ù‡ Ø¬Ø¯ÛŒØ¯ Ø¨Ø§Ø´Ø¯ Ú†Ù‡ Ù…ÙˆØ¬ÙˆØ¯)
                new_docs.append(
                    Document(page_content=content, metadata={"source": f.name})
                )

            if new_docs:
                splitter = RecursiveCharacterTextSplitter(
                    chunk_size=1000, chunk_overlap=200
                )
                splits = splitter.split_documents(new_docs)

                # Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡â€ŒÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³
                if st.session_state.vectorstore is None:
                    st.session_state.vectorstore = FAISS.from_documents(
                        splits, embeddings
                    )
                else:
                    st.session_state.vectorstore.add_documents(splits)

                # --- Ø¹Ù…Ù„ÛŒØ§Øª Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§ÛŒÙ…Ù† (Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ù‡ CPU Ùˆ Ø¨Ø§Ø²Ú¯Ø´Øª Ø¨Ù‡ GPU) ---
                gpu_index = st.session_state.vectorstore.index
                st.session_state.vectorstore.index = faiss.index_gpu_to_cpu(gpu_index)
                st.session_state.vectorstore.save_local(str(INDEX_PATH))
                st.session_state.vectorstore.index = gpu_index
                # ---------------------------------------------------------

                status.update(label="Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡â€ŒØ±ÙˆØ² Ø´Ø¯!", state="complete")
                st.rerun()
            else:
                status.update(label="ÙØ§ÛŒÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª.", state="error")

if "messages" not in st.session_state:
    st.session_state.messages = []

for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.markdown(
            # f'<div dir="rtl" style="text-align:right">{m["content"]}</div>',
            f'{m["content"]}',
            unsafe_allow_html=True,
        )

if prompt := st.chat_input("Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø§ Ø¬Ø²Ø¦ÛŒØ§Øª Ø¨Ù¾Ø±Ø³ÛŒØ¯..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(
            f'<div dir="rtl" style="text-align:right">{prompt}</div>',
            unsafe_allow_html=True,
        )

    with st.chat_message("assistant"):
        if st.session_state.vectorstore:
            resp_placeholder = st.empty()

            # Û±. Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ùˆ Ø±Ù†Ú©ÛŒÙ†Ú¯ (Ø¨Ø³ÛŒØ§Ø± Ø¯Ù‚ÛŒÙ‚)
            docs = st.session_state.vectorstore.similarity_search(prompt, k=15)
            pairs = [[prompt, d.page_content] for d in docs]
            inputs = rerank_t(
                pairs,
                padding=True,
                truncation=True,
                return_tensors="pt",
                max_length=512,
            ).to("cuda:1")

            with torch.no_grad():
                scores = rerank_m(**inputs).logits.view(-1).float()
                # Ø§Ù†ØªØ®Ø§Ø¨ Û±Û° Ù‚Ø·Ø¹Ù‡ Ø¨Ø±ØªØ± Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø´ØªÙ† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø­Ø¯Ø§Ú©Ø«Ø±ÛŒ
                best_indices = torch.argsort(scores, descending=True)[:10]
                selected_docs = [docs[i] for i in best_indices]
                context_text = "\n\n".join([d.page_content for d in selected_docs])
                sources = list(
                    set([d.metadata.get("source", "Ù†Ø§Ø´Ù†Ø§Ø³") for d in selected_docs])
                )

            # Û². Ø³Ø§Ø®Øª Ù¾Ø±ÙˆÙ…Ù¾Øª Ù…Ù‡Ù†Ø¯Ø³ÛŒ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø® Ø·ÙˆÙ„Ø§Ù†ÛŒ
            # Ù†Ú©ØªÙ‡: Ø§Ú¯Ø± Ú©Ø§Ø±Ø¨Ø± "Ù…ØªÙ† Ú©Ø§Ù…Ù„" Ø®ÙˆØ§Ø³ØªØŒ Ù…Ø³ØªÙ‚ÛŒÙ… Ø§Ø² Ù…ØªØ§Ø¯ÛŒØªØ§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†ÛŒÙ…
            if any(word in prompt for word in ["Ù…ØªÙ† Ú©Ø§Ù…Ù„", "Ú©Ù„ ÙØ§ÛŒÙ„", "ØªÙ…ÙˆÙ… Ù…ØªÙ†"]):
                full_raw = ""
                for meta_file in METADATA_DIR.glob("*.json"):
                    with open(meta_file, "r", encoding="utf-8") as m:
                        data = json.load(m)
                        full_raw += f"\n--- Ù…Ø­ØªÙˆØ§ÛŒ ÙØ§ÛŒÙ„: {data['name']} ---\n{data['full_content']}\n"
                full_ans = full_raw if full_raw else "Ù…ØªÙ†ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯."
            else:
                # Ø¯Ø³ØªÙˆØ± ØµØ±ÛŒØ­ Ø¨Ù‡ Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ù‡ÛŒØ² Ø§Ø² Ù¾Ø§Ø³Ø® Ú©ÙˆØªØ§Ù‡
                enhanced_prompt = f"""Ø´Ù…Ø§ ÛŒÚ© Ú©Ø§Ø±Ø´Ù†Ø§Ø³ Ø®Ø¨Ø±Ù‡ ØªØ­Ù„ÛŒÙ„ Ù…Ø­ØªÙˆØ§ Ù‡Ø³ØªÛŒØ¯.
                Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø²ÛŒØ±ØŒ ÛŒÚ© Ù¾Ø§Ø³Ø® **Ø¨Ø³ÛŒØ§Ø± Ø¬Ø§Ù…Ø¹ØŒ Ù…ÙØµÙ„ Ùˆ Ø¨Ø§ ØªÙ…Ø§Ù… Ø¬Ø²Ø¦ÛŒØ§Øª** Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯.
                ØªØ£Ú©ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ù…: Ù¾Ø§Ø³Ø® Ù†Ø¨Ø§ÛŒØ¯ Ú©ÙˆØªØ§Ù‡ Ø¨Ø§Ø´Ø¯. ØªÙ…Ø§Ù… Ù†Ú©Ø§Øª Ù…Ù‡Ù… Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù…ØªÙ† Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ùˆ ØªØ­Ù„ÛŒÙ„ Ú©Ù†ÛŒØ¯.
                Ø§Ú¯Ø± Ø³ÙˆØ§Ù„ÛŒ Ù¾Ø±Ø³ÛŒØ¯Ù‡ Ø´Ø¯Ù‡ Ú©Ù‡ Ø¯Ø± Ù…ØªÙ† Ù†ÛŒØ³ØªØŒ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯: 'Ø¯Ø± Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¨Ù‡ Ø§ÛŒÙ† Ù…ÙˆØ±Ø¯ Ø§Ø´Ø§Ø±Ù‡ Ù†Ø´Ø¯Ù‡'.

                Ù…Ø­ØªÙˆØ§ÛŒ Ø§Ø³Ù†Ø§Ø¯:
                {context_text}

                Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±: {prompt}
                
                Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ùˆ Ú©Ø§Ù…Ù„:"""

                full_ans = ""
                for chunk in llm_engine.stream(enhanced_prompt):
                    # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù†ÙˆÛŒØ²Ù‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ
                    chunk = re.sub(
                        r"(System:|Assistant:|Human:|User:).*?",
                        "",
                        chunk,
                        flags=re.IGNORECASE,
                    )
                    full_ans += chunk
                    resp_placeholder.markdown(
                        # f'<p dir="rtl" style="text-align:right">{full_ans} â–Œ</p>',
                        f"{full_ans}",
                        unsafe_allow_html=True,
                    )

            # Û³. Ù†Ù…Ø§ÛŒØ´ Ù†Ù‡Ø§ÛŒÛŒ Ù‡Ù…Ø±Ø§Ù‡ Ø¨Ø§ ØªÚ¯ Ù…Ù†Ø§Ø¨Ø¹
            source_html = " ".join(
                [
                    f'<span style="background:#1e293b; padding:2px 8px; border-radius:5px; font-size:12px; margin-right:5px;">ğŸ“„ {s}</span>'
                    for s in sources
                ]
            )
            final_output = f'<div dir="rtl" style="text-align:right;">{full_ans}<br><br><hr>{source_html}</div>'
            resp_placeholder.markdown(final_output, unsafe_allow_html=True)

            st.session_state.messages.append({"role": "assistant", "content": full_ans})
            torch.cuda.empty_cache()
            gc.collect()
        else:
            st.warning(
                "Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø®Ø§Ù„ÛŒ Ø§Ø³Øª! Ù„Ø·ÙØ§Ù‹ Ø§Ø¨ØªØ¯Ø§ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø¯Ø± Ø³Ø§ÛŒØ¯Ø¨Ø§Ø± Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯."
            )
